{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f938540",
   "metadata": {},
   "source": [
    "# MLOps Coloring Book\n",
    "\n",
    "This notebook may be used for demonstration of the 1-ClickMLOps tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9222df33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 1.8.14\n"
     ]
    }
   ],
   "source": [
    "# Check that package is installed correctly. The KFP SDK version should be >=1.6:\n",
    "!python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2219ee67",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "164ff1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.dsl import pipeline, component, Artifact, Dataset, Input, Metrics, Model, Output, InputPath, OutputPath\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import aiplatform_v1\n",
    "import datetime\n",
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "from utils import OneClickMLOps\n",
    "\n",
    "@register_cell_magic\n",
    "def execute_and_save(file, cell):\n",
    "    'Run and save python code block to a file'\n",
    "    with open(file, 'wt') as fd:\n",
    "        fd.write(cell)\n",
    "    code = compile(cell, file, 'exec')\n",
    "    exec(code, globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac0c712-acdd-4830-a9bd-44f41eaa590c",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94a96074-761f-40f0-84bb-adfcb9bf6911",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"google-cloud-bigquery\", \n",
    "        \"pandas\",\n",
    "        \"pyarrow\",\n",
    "        \"db_dtypes\"\n",
    "    ],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"create_dataset.yaml\"\n",
    ")\n",
    "def create_dataset(\n",
    "    bq_table: str,\n",
    "    output_data_path: OutputPath(\"Dataset\"),\n",
    "    project: str\n",
    "):\n",
    "    from google.cloud import bigquery\n",
    "    import pandas as pd\n",
    "    bq_client = bigquery.Client(project=project)\n",
    "\n",
    "\n",
    "    def get_query(bq_input_table: str) -> str:\n",
    "        \"\"\"Generates BQ Query to read data.\n",
    "\n",
    "        Args:\n",
    "        bq_input_table: The full name of the bq input table to be read into\n",
    "        the dataframe (e.g. <project>.<dataset>.<table>)\n",
    "        Returns: A BQ query string.\n",
    "        \"\"\"\n",
    "        return f\"\"\"\n",
    "        SELECT *\n",
    "        FROM `{bq_input_table}`\n",
    "        \"\"\"\n",
    "\n",
    "    def load_bq_data(query: str, client: bigquery.Client) -> pd.DataFrame:\n",
    "        \"\"\"Loads data from bq into a Pandas Dataframe for EDA.\n",
    "        Args:\n",
    "        query: BQ Query to generate data.\n",
    "        client: BQ Client used to execute query.\n",
    "        Returns:\n",
    "        pd.DataFrame: A dataframe with the requested data.\n",
    "        \"\"\"\n",
    "        df = client.query(query).to_dataframe()\n",
    "        return df\n",
    "\n",
    "    dataframe = load_bq_data(get_query(bq_table), bq_client)\n",
    "    dataframe.to_csv(output_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4476cb4-91c5-42ff-a500-8cc275fedbd1",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5371be73-db3f-4d79-bde7-94fcd5ea13b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"scikit-learn\",\n",
    "        \"pandas\",\n",
    "        \"joblib\",\n",
    "        \"tensorflow\"\n",
    "    ],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"train_model.yaml\",\n",
    ")\n",
    "def train_model(\n",
    "    output_model_directory: str,\n",
    "    dataset: Input[Dataset],\n",
    "    metrics: Output[Metrics],\n",
    "    model: Output[Model]\n",
    "):\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from joblib import dump\n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "    import pickle\n",
    "    import os\n",
    "    \n",
    "    def save_model(model, uri):\n",
    "        \"\"\"Saves a model to uri.\"\"\"\n",
    "        with tf.io.gfile.GFile(uri, 'w') as f:\n",
    "            pickle.dump(model, f)\n",
    "    \n",
    "    df = pd.read_csv(dataset.path)\n",
    "    labels = df.pop(\"Class\").tolist()\n",
    "    data = df.values.tolist()\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, labels)\n",
    "    skmodel = DecisionTreeClassifier()\n",
    "    skmodel.fit(x_train,y_train)\n",
    "    score = skmodel.score(x_test,y_test)\n",
    "    print('accuracy is:',score)\n",
    "    metrics.log_metric(\"accuracy\",(score * 100.0))\n",
    "    metrics.log_metric(\"framework\", \"Scikit Learn\")\n",
    "    metrics.log_metric(\"dataset_size\", len(df))\n",
    "\n",
    "    output_uri = os.path.join(output_model_directory, f'model.pkl')\n",
    "    save_model(skmodel, output_uri)\n",
    "    model.path = output_model_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ed1eaa",
   "metadata": {},
   "source": [
    "## Uploading & Deploying the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47377d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"google-cloud-aiplatform\"\n",
    "    ],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"deploy_model.yaml\",\n",
    ")\n",
    "def deploy_model(\n",
    "    model: Input[Model],\n",
    "    project: str,\n",
    "    region: str,\n",
    "    vertex_endpoint: Output[Artifact],\n",
    "    vertex_model: Output[Model]\n",
    "):\n",
    "    from google.cloud import aiplatform\n",
    "    aiplatform.init(project=project, location=region)\n",
    "    deployed_model = aiplatform.Model.upload(\n",
    "        display_name=\"beans-model-pipeline\",\n",
    "        artifact_uri = model.uri,\n",
    "        serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-24:latest\"\n",
    "    )\n",
    "    endpoint = deployed_model.deploy(machine_type=\"n1-standard-4\")\n",
    "    vertex_endpoint.uri = endpoint.resource_name\n",
    "    vertex_model.uri = deployed_model.resource_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d96dcb-a020-4bab-b0e3-1e32f6b2aecf",
   "metadata": {},
   "source": [
    "## Define and Run the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a36e0154",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%execute_and_save pipeline.py \n",
    "@dsl.pipeline(name='training-pipeline')\n",
    "def pipeline(bq_table: str,\n",
    "             output_model_directory: str,\n",
    "             project: str,\n",
    "             region: str,\n",
    "            ):\n",
    "    \n",
    "    dataset_task = create_dataset(\n",
    "        bq_table=bq_table, \n",
    "        project=project)\n",
    "\n",
    "    model_task = train_model(\n",
    "        output_model_directory=output_model_directory,\n",
    "        dataset=dataset_task.output)\n",
    "\n",
    "    deploy_task = deploy_model(\n",
    "        model=model_task.outputs[\"model\"],\n",
    "        project=project,\n",
    "        region=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e5b59f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_params = {\n",
    "    \"bq_table\": \"sandbox-srastatter.mlops_boxer_test.dry-beans\",\n",
    "    \"output_model_directory\": f\"gs://mlops-boxer-test/trained_models/{datetime.datetime.now()}\",\n",
    "    \"project\": \"sandbox-srastatter\",\n",
    "    \"region\": \"us-central1\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ef279e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;32m BUILDING COMPONENTS \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 29 file(s) totalling 64.0 KiB before compression.\n",
      "Uploading tarball of [..] to [gs://sandbox-srastatter_cloudbuild/source/1670946930.221125-7e83a6e9f43645f08e6436e8afa8ebf2.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/sandbox-srastatter/locations/global/builds/359701bb-4085-443f-bc3c-abb9cf53700f].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/359701bb-4085-443f-bc3c-abb9cf53700f?project=1006819402307 ].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"359701bb-4085-443f-bc3c-abb9cf53700f\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://sandbox-srastatter_cloudbuild/source/1670946930.221125-7e83a6e9f43645f08e6436e8afa8ebf2.tgz#1670946930584839\n",
      "Copying gs://sandbox-srastatter_cloudbuild/source/1670946930.221125-7e83a6e9f43645f08e6436e8afa8ebf2.tgz#1670946930584839...\n",
      "/ [1 files][ 14.6 KiB/ 14.6 KiB]                                                \n",
      "Operation completed over 1 objects/14.6 KiB.\n",
      "BUILD\n",
      "Starting Step #0 - \"Build component: deploy_model\"\n",
      "Step #0 - \"Build component: deploy_model\": Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Step #0 - \"Build component: deploy_model\": Sending build context to Docker daemon  6.656kB\n",
      "Step #0 - \"Build component: deploy_model\": Step 1/7 : FROM python:3.9\n",
      "Step #0 - \"Build component: deploy_model\": 3.9: Pulling from library/python\n",
      "Step #0 - \"Build component: deploy_model\": f2f58072e9ed: Pulling fs layer\n",
      "Step #0 - \"Build component: deploy_model\": 5c8cfbf51e6e: Pulling fs layer\n",
      "Step #0 - \"Build component: deploy_model\": aa3a609d1579: Pulling fs layer\n",
      "Step #0 - \"Build component: deploy_model\": 094e7d9bb04e: Pulling fs layer\n",
      "Step #0 - \"Build component: deploy_model\": 2cbfd734f382: Pulling fs layer\n",
      "Step #0 - \"Build component: deploy_model\": aa86ac293d0f: Pulling fs layer\n",
      "Step #0 - \"Build component: deploy_model\": ea442e3d4174: Pulling fs layer\n",
      "Step #0 - \"Build component: deploy_model\": c662908b49d7: Pulling fs layer\n",
      "Step #0 - \"Build component: deploy_model\": f7e80cce9b62: Pulling fs layer\n",
      "Step #0 - \"Build component: deploy_model\": aa86ac293d0f: Waiting\n",
      "Step #0 - \"Build component: deploy_model\": ea442e3d4174: Waiting\n",
      "Step #0 - \"Build component: deploy_model\": c662908b49d7: Waiting\n",
      "Step #0 - \"Build component: deploy_model\": f7e80cce9b62: Waiting\n",
      "Step #0 - \"Build component: deploy_model\": 094e7d9bb04e: Waiting\n",
      "Step #0 - \"Build component: deploy_model\": 2cbfd734f382: Waiting\n",
      "Step #0 - \"Build component: deploy_model\": 5c8cfbf51e6e: Verifying Checksum\n",
      "Step #0 - \"Build component: deploy_model\": 5c8cfbf51e6e: Download complete\n",
      "Step #0 - \"Build component: deploy_model\": aa3a609d1579: Verifying Checksum\n",
      "Step #0 - \"Build component: deploy_model\": aa3a609d1579: Download complete\n",
      "Step #0 - \"Build component: deploy_model\": f2f58072e9ed: Verifying Checksum\n",
      "Step #0 - \"Build component: deploy_model\": f2f58072e9ed: Download complete\n",
      "Step #0 - \"Build component: deploy_model\": 094e7d9bb04e: Verifying Checksum\n",
      "Step #0 - \"Build component: deploy_model\": 094e7d9bb04e: Download complete\n",
      "Step #0 - \"Build component: deploy_model\": aa86ac293d0f: Verifying Checksum\n",
      "Step #0 - \"Build component: deploy_model\": aa86ac293d0f: Download complete\n",
      "Step #0 - \"Build component: deploy_model\": c662908b49d7: Download complete\n",
      "Step #0 - \"Build component: deploy_model\": f7e80cce9b62: Verifying Checksum\n",
      "Step #0 - \"Build component: deploy_model\": f7e80cce9b62: Download complete\n",
      "Step #0 - \"Build component: deploy_model\": ea442e3d4174: Verifying Checksum\n",
      "Step #0 - \"Build component: deploy_model\": ea442e3d4174: Download complete\n",
      "Step #0 - \"Build component: deploy_model\": 2cbfd734f382: Verifying Checksum\n",
      "Step #0 - \"Build component: deploy_model\": 2cbfd734f382: Download complete\n",
      "Step #0 - \"Build component: deploy_model\": f2f58072e9ed: Pull complete\n",
      "Step #0 - \"Build component: deploy_model\": 5c8cfbf51e6e: Pull complete\n",
      "Step #0 - \"Build component: deploy_model\": aa3a609d1579: Pull complete\n",
      "Step #0 - \"Build component: deploy_model\": 094e7d9bb04e: Pull complete\n",
      "Step #0 - \"Build component: deploy_model\": 2cbfd734f382: Pull complete\n",
      "Step #0 - \"Build component: deploy_model\": aa86ac293d0f: Pull complete\n",
      "Step #0 - \"Build component: deploy_model\": ea442e3d4174: Pull complete\n",
      "Step #0 - \"Build component: deploy_model\": c662908b49d7: Pull complete\n",
      "Step #0 - \"Build component: deploy_model\": f7e80cce9b62: Pull complete\n",
      "Step #0 - \"Build component: deploy_model\": Digest: sha256:929da7c4e1285e844e70e901267059f63ea958778695a867111a77eaf09700ff\n",
      "Step #0 - \"Build component: deploy_model\": Status: Downloaded newer image for python:3.9\n",
      "Step #0 - \"Build component: deploy_model\":  ---> 7d357ce6a803\n",
      "Step #0 - \"Build component: deploy_model\": Step 2/7 : RUN python -m pip install --upgrade pip\n",
      "Step #0 - \"Build component: deploy_model\":  ---> Running in aa9b3db4fc98\n",
      "Step #0 - \"Build component: deploy_model\": Requirement already satisfied: pip in /usr/local/lib/python3.9/site-packages (22.0.4)\n",
      "Step #0 - \"Build component: deploy_model\": Collecting pip\n",
      "Step #0 - \"Build component: deploy_model\":   Downloading pip-22.3.1-py3-none-any.whl (2.1 MB)\n",
      "Step #0 - \"Build component: deploy_model\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 38.4 MB/s eta 0:00:00\n",
      "Step #0 - \"Build component: deploy_model\": Installing collected packages: pip\n",
      "Step #0 - \"Build component: deploy_model\":   Attempting uninstall: pip\n",
      "Step #0 - \"Build component: deploy_model\":     Found existing installation: pip 22.0.4\n",
      "Step #0 - \"Build component: deploy_model\":     Uninstalling pip-22.0.4:\n",
      "Step #0 - \"Build component: deploy_model\":       Successfully uninstalled pip-22.0.4\n",
      "Step #0 - \"Build component: deploy_model\": Successfully installed pip-22.3.1\n",
      "Step #0 - \"Build component: deploy_model\": \u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "Step #0 - \"Build component: deploy_model\": \u001b[0mRemoving intermediate container aa9b3db4fc98\n",
      "Step #0 - \"Build component: deploy_model\":  ---> e478cf23a423\n",
      "Step #0 - \"Build component: deploy_model\": Step 3/7 : COPY requirements.txt .\n",
      "Step #0 - \"Build component: deploy_model\":  ---> ad054462bccb\n",
      "Step #0 - \"Build component: deploy_model\": Step 4/7 : RUN python -m pip install -r     requirements.txt --quiet --no-cache-dir     && rm -f requirements.txt\n",
      "Step #0 - \"Build component: deploy_model\":  ---> Running in e45469325c86\n",
      "Step #0 - \"Build component: deploy_model\": \u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "Step #0 - \"Build component: deploy_model\": \u001b[0mRemoving intermediate container e45469325c86\n",
      "Step #0 - \"Build component: deploy_model\":  ---> 119366d5dfe2\n",
      "Step #0 - \"Build component: deploy_model\": Step 5/7 : WORKDIR /\n",
      "Step #0 - \"Build component: deploy_model\":  ---> Running in f970dfd02737\n",
      "Step #0 - \"Build component: deploy_model\": Removing intermediate container f970dfd02737\n",
      "Step #0 - \"Build component: deploy_model\":  ---> d1828ddbfcef\n",
      "Step #0 - \"Build component: deploy_model\": Step 6/7 : COPY trainer /trainer\n",
      "Step #0 - \"Build component: deploy_model\":  ---> 9076ca5e53ec\n",
      "Step #0 - \"Build component: deploy_model\": Step 7/7 : ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]\n",
      "Step #0 - \"Build component: deploy_model\":  ---> Running in d18a1795ea38\n",
      "Step #0 - \"Build component: deploy_model\": Removing intermediate container d18a1795ea38\n",
      "Step #0 - \"Build component: deploy_model\":  ---> 9848479da6da\n",
      "Step #0 - \"Build component: deploy_model\": Successfully built 9848479da6da\n",
      "Step #0 - \"Build component: deploy_model\": Successfully tagged us-central1-docker.pkg.dev/sandbox-srastatter/vertex-mlops-af/components/deploy_model:latest\n",
      "Finished Step #0 - \"Build component: deploy_model\"\n",
      "Starting Step #1 - \"Build component: train_model\"\n",
      "Step #1 - \"Build component: train_model\": Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Step #1 - \"Build component: train_model\": Sending build context to Docker daemon  7.168kB\n",
      "Step #1 - \"Build component: train_model\": Step 1/7 : FROM python:3.9\n",
      "Step #1 - \"Build component: train_model\":  ---> 7d357ce6a803\n",
      "Step #1 - \"Build component: train_model\": Step 2/7 : RUN python -m pip install --upgrade pip\n",
      "Step #1 - \"Build component: train_model\":  ---> Using cache\n",
      "Step #1 - \"Build component: train_model\":  ---> e478cf23a423\n",
      "Step #1 - \"Build component: train_model\": Step 3/7 : COPY requirements.txt .\n",
      "Step #1 - \"Build component: train_model\":  ---> 005a8073139e\n",
      "Step #1 - \"Build component: train_model\": Step 4/7 : RUN python -m pip install -r     requirements.txt --quiet --no-cache-dir     && rm -f requirements.txt\n",
      "Step #1 - \"Build component: train_model\":  ---> Running in a03623c7cf17\n",
      "Step #1 - \"Build component: train_model\": \u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "Step #1 - \"Build component: train_model\": \u001b[0mRemoving intermediate container a03623c7cf17\n",
      "Step #1 - \"Build component: train_model\":  ---> 7deea34e0164\n",
      "Step #1 - \"Build component: train_model\": Step 5/7 : WORKDIR /\n",
      "Step #1 - \"Build component: train_model\":  ---> Running in 365059b16f03\n",
      "Step #1 - \"Build component: train_model\": Removing intermediate container 365059b16f03\n",
      "Step #1 - \"Build component: train_model\":  ---> 6ee1634cbb2e\n",
      "Step #1 - \"Build component: train_model\": Step 6/7 : COPY trainer /trainer\n",
      "Step #1 - \"Build component: train_model\":  ---> c3fe965e6f1f\n",
      "Step #1 - \"Build component: train_model\": Step 7/7 : ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]\n",
      "Step #1 - \"Build component: train_model\":  ---> Running in 6cd84a457442\n",
      "Step #1 - \"Build component: train_model\": Removing intermediate container 6cd84a457442\n",
      "Step #1 - \"Build component: train_model\":  ---> 44de44b66c09\n",
      "Step #1 - \"Build component: train_model\": Successfully built 44de44b66c09\n",
      "Step #1 - \"Build component: train_model\": Successfully tagged us-central1-docker.pkg.dev/sandbox-srastatter/vertex-mlops-af/components/train_model:latest\n",
      "Finished Step #1 - \"Build component: train_model\"\n",
      "Starting Step #2 - \"Build component: create_dataset\"\n",
      "Step #2 - \"Build component: create_dataset\": Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Step #2 - \"Build component: create_dataset\": Sending build context to Docker daemon  7.168kB\n",
      "Step #2 - \"Build component: create_dataset\": Step 1/7 : FROM python:3.9\n",
      "Step #2 - \"Build component: create_dataset\":  ---> 7d357ce6a803\n",
      "Step #2 - \"Build component: create_dataset\": Step 2/7 : RUN python -m pip install --upgrade pip\n",
      "Step #2 - \"Build component: create_dataset\":  ---> Using cache\n",
      "Step #2 - \"Build component: create_dataset\":  ---> e478cf23a423\n",
      "Step #2 - \"Build component: create_dataset\": Step 3/7 : COPY requirements.txt .\n",
      "Step #2 - \"Build component: create_dataset\":  ---> f8a19cd386b2\n",
      "Step #2 - \"Build component: create_dataset\": Step 4/7 : RUN python -m pip install -r     requirements.txt --quiet --no-cache-dir     && rm -f requirements.txt\n",
      "Step #2 - \"Build component: create_dataset\":  ---> Running in 7dc7d0cec420\n",
      "Step #2 - \"Build component: create_dataset\": \u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "Step #2 - \"Build component: create_dataset\": \u001b[0mRemoving intermediate container 7dc7d0cec420\n",
      "Step #2 - \"Build component: create_dataset\":  ---> 17fd98b5e6eb\n",
      "Step #2 - \"Build component: create_dataset\": Step 5/7 : WORKDIR /\n",
      "Step #2 - \"Build component: create_dataset\":  ---> Running in dd5006b40a67\n",
      "Step #2 - \"Build component: create_dataset\": Removing intermediate container dd5006b40a67\n",
      "Step #2 - \"Build component: create_dataset\":  ---> 78b5f6c92287\n",
      "Step #2 - \"Build component: create_dataset\": Step 6/7 : COPY trainer /trainer\n",
      "Step #2 - \"Build component: create_dataset\":  ---> 4fe25d1b206b\n",
      "Step #2 - \"Build component: create_dataset\": Step 7/7 : ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]\n",
      "Step #2 - \"Build component: create_dataset\":  ---> Running in 28c94fd5ff02\n",
      "Step #2 - \"Build component: create_dataset\": Removing intermediate container 28c94fd5ff02\n",
      "Step #2 - \"Build component: create_dataset\":  ---> d7eea306f48b\n",
      "Step #2 - \"Build component: create_dataset\": Successfully built d7eea306f48b\n",
      "Step #2 - \"Build component: create_dataset\": Successfully tagged us-central1-docker.pkg.dev/sandbox-srastatter/vertex-mlops-af/components/create_dataset:latest\n",
      "Finished Step #2 - \"Build component: create_dataset\"\n",
      "PUSH\n",
      "Pushing us-central1-docker.pkg.dev/sandbox-srastatter/vertex-mlops-af/components/deploy_model:latest\n",
      "The push refers to repository [us-central1-docker.pkg.dev/sandbox-srastatter/vertex-mlops-af/components/deploy_model]\n",
      "d20a574c16c9: Preparing\n",
      "4e4018e60606: Preparing\n",
      "9e91c7f6172e: Preparing\n",
      "ae1fe9ec8566: Preparing\n",
      "f95cd016d6e3: Preparing\n",
      "79b4fe16a228: Preparing\n",
      "792cd6061bec: Preparing\n",
      "1cad4dc57058: Preparing\n",
      "4ff8844d474a: Preparing\n",
      "b77487480ddb: Preparing\n",
      "cd247c0fb37b: Preparing\n",
      "cfdd5c3bd77e: Preparing\n",
      "870a241bfebd: Preparing\n",
      "1cad4dc57058: Waiting\n",
      "4ff8844d474a: Waiting\n",
      "b77487480ddb: Waiting\n",
      "cd247c0fb37b: Waiting\n",
      "cfdd5c3bd77e: Waiting\n",
      "870a241bfebd: Waiting\n",
      "79b4fe16a228: Waiting\n",
      "792cd6061bec: Waiting\n",
      "9e91c7f6172e: Pushed\n",
      "d20a574c16c9: Pushed\n",
      "ae1fe9ec8566: Pushed\n",
      "f95cd016d6e3: Pushed\n",
      "79b4fe16a228: Pushed\n",
      "1cad4dc57058: Pushed\n",
      "792cd6061bec: Pushed\n",
      "cd247c0fb37b: Pushed\n",
      "cfdd5c3bd77e: Pushed\n",
      "4e4018e60606: Pushed\n",
      "b77487480ddb: Pushed\n",
      "870a241bfebd: Pushed\n",
      "4ff8844d474a: Pushed\n",
      "latest: digest: sha256:36c2b33e61ea1a3208cd558d4cf935c1305d4ddf99270fcbff5c4d66d7dd6be7 size: 3055\n",
      "Pushing us-central1-docker.pkg.dev/sandbox-srastatter/vertex-mlops-af/components/train_model:latest\n",
      "The push refers to repository [us-central1-docker.pkg.dev/sandbox-srastatter/vertex-mlops-af/components/train_model]\n",
      "64e78e106c5d: Preparing\n",
      "9e28fb36c333: Preparing\n",
      "a6509ad2646b: Preparing\n",
      "ae1fe9ec8566: Preparing\n",
      "f95cd016d6e3: Preparing\n",
      "79b4fe16a228: Preparing\n",
      "792cd6061bec: Preparing\n",
      "1cad4dc57058: Preparing\n",
      "4ff8844d474a: Preparing\n",
      "b77487480ddb: Preparing\n",
      "cd247c0fb37b: Preparing\n",
      "cfdd5c3bd77e: Preparing\n",
      "870a241bfebd: Preparing\n",
      "79b4fe16a228: Waiting\n",
      "792cd6061bec: Waiting\n",
      "cd247c0fb37b: Waiting\n",
      "1cad4dc57058: Waiting\n",
      "4ff8844d474a: Waiting\n",
      "b77487480ddb: Waiting\n",
      "cfdd5c3bd77e: Waiting\n",
      "870a241bfebd: Waiting\n",
      "ae1fe9ec8566: Layer already exists\n",
      "79b4fe16a228: Layer already exists\n",
      "f95cd016d6e3: Layer already exists\n",
      "792cd6061bec: Layer already exists\n",
      "1cad4dc57058: Layer already exists\n",
      "4ff8844d474a: Layer already exists\n",
      "b77487480ddb: Layer already exists\n",
      "cd247c0fb37b: Layer already exists\n",
      "cfdd5c3bd77e: Layer already exists\n",
      "870a241bfebd: Layer already exists\n",
      "a6509ad2646b: Pushed\n",
      "64e78e106c5d: Pushed\n",
      "9e28fb36c333: Pushed\n",
      "latest: digest: sha256:2eb1e4073295d97f1d54dd9fd9aed5e0f1ba9bc69a21b63976016036e10ee0cf size: 3056\n",
      "Pushing us-central1-docker.pkg.dev/sandbox-srastatter/vertex-mlops-af/components/create_dataset:latest\n",
      "The push refers to repository [us-central1-docker.pkg.dev/sandbox-srastatter/vertex-mlops-af/components/create_dataset]\n",
      "a54c4ba6490a: Preparing\n",
      "24c3698059a4: Preparing\n",
      "a979125c8a51: Preparing\n",
      "ae1fe9ec8566: Preparing\n",
      "f95cd016d6e3: Preparing\n",
      "79b4fe16a228: Preparing\n",
      "792cd6061bec: Preparing\n",
      "1cad4dc57058: Preparing\n",
      "4ff8844d474a: Preparing\n",
      "b77487480ddb: Preparing\n",
      "cd247c0fb37b: Preparing\n",
      "cfdd5c3bd77e: Preparing\n",
      "870a241bfebd: Preparing\n",
      "79b4fe16a228: Waiting\n",
      "b77487480ddb: Waiting\n",
      "cd247c0fb37b: Waiting\n",
      "cfdd5c3bd77e: Waiting\n",
      "870a241bfebd: Waiting\n",
      "792cd6061bec: Waiting\n",
      "1cad4dc57058: Waiting\n",
      "4ff8844d474a: Waiting\n",
      "f95cd016d6e3: Layer already exists\n",
      "ae1fe9ec8566: Layer already exists\n",
      "792cd6061bec: Layer already exists\n",
      "79b4fe16a228: Layer already exists\n",
      "4ff8844d474a: Layer already exists\n",
      "b77487480ddb: Layer already exists\n",
      "1cad4dc57058: Layer already exists\n",
      "cd247c0fb37b: Layer already exists\n",
      "cfdd5c3bd77e: Layer already exists\n",
      "870a241bfebd: Layer already exists\n",
      "a979125c8a51: Pushed\n",
      "a54c4ba6490a: Pushed\n",
      "24c3698059a4: Pushed\n",
      "latest: digest: sha256:a370e1506272e4ce85dd361725ba6a979783386b531f723fbdc7a09760750528 size: 3056\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                            IMAGES                                                                                           STATUS\n",
      "359701bb-4085-443f-bc3c-abb9cf53700f  2022-12-13T15:55:30+00:00  9M57S     gs://sandbox-srastatter_cloudbuild/source/1670946930.221125-7e83a6e9f43645f08e6436e8afa8ebf2.tgz  us-central1-docker.pkg.dev/sandbox-srastatter/vertex-mlops-af/components/deploy_model (+5 more)  SUCCESS\n",
      "\u001b[0;32m BUILDING PIPELINE SPEC \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srastatter/Library/Python/3.9/lib/python/site-packages/kfp/v2/compiler/compiler.py:1290: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;32m RUN PIPELINE JOB \u001b[0m\n",
      "Creating PipelineJob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob created. Resource name: projects/1006819402307/locations/us-central1/pipelineJobs/training-pipeline-20221213110537\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/1006819402307/locations/us-central1/pipelineJobs/training-pipeline-20221213110537')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/training-pipeline-20221213110537?project=1006819402307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/1006819402307/locations/us-central1/pipelineJobs/training-pipeline-20221213110537\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/1006819402307/locations/us-central1/pipelineJobs/training-pipeline-20221213110537')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/training-pipeline-20221213110537?project=1006819402307\n"
     ]
    }
   ],
   "source": [
    "OneClickMLOps.go(project_id='sandbox-srastatter', pipeline_params=pipeline_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e2f07ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "OneClickMLOps.generate(project_id='sandbox-srastatter',\n",
    "                       af_registry_name='mlops-boxer-test',\n",
    "                       af_registry_location='us-central1',\n",
    "                       gs_bucket_location='us-central1',\n",
    "                       gs_bucket_name='mlops-boxer-test',\n",
    "                       pipeline_params=pipeline_params)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m91"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
