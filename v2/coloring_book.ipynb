{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f938540",
   "metadata": {},
   "source": [
    "# MLOps Coloring Book\n",
    "\n",
    "This notebook may be used for demonstration of the 1-ClickMLOps tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2219ee67",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51e1d224",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "from utils import OneClickMLOps\n",
    "# consider dumping files to a tmpfiles dir\n",
    "@register_cell_magic\n",
    "def imports(_, cell):\n",
    "    'Run and save python code block to a file'\n",
    "    file = '.imports.py'\n",
    "    with open(file, 'wt') as fd:\n",
    "        fd.write(cell)\n",
    "    code = compile(cell, file, 'exec')\n",
    "    exec(code, globals())\n",
    "\n",
    "@register_cell_magic\n",
    "def define(_, cell):\n",
    "    'Run and save python code block to a file'\n",
    "    file = '.cell.py'\n",
    "    with open(file, 'wt') as fd:\n",
    "        fd.write(cell)\n",
    "    code_to_exec = cell[cell.find(\"OneClickMLOps.makeComponent(\"):cell.find(\")\")+1]\n",
    "    code = compile(code_to_exec, file, 'exec')\n",
    "    exec(code, globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "164ff1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%imports\n",
    "import json\n",
    "import pandas as pd\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import aiplatform_v1\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import datetime\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from joblib import dump\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bde3b48-8012-4487-bbc4-b9a6198e9070",
   "metadata": {},
   "source": [
    "## Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "678f3b64-28fb-4751-8286-16efb924efee",
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_table = \"sandbox-srastatter.mlops_boxer_test.dry-beans\"\n",
    "model_directory = f\"gs://mlops-boxer-test/trained_models/{datetime.datetime.now()}\"\n",
    "data_path = \"gs://mlops-boxer-test/data\"\n",
    "project_id = \"sandbox-srastatter\"\n",
    "region = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac0c712-acdd-4830-a9bd-44f41eaa590c",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94a96074-761f-40f0-84bb-adfcb9bf6911",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%define\n",
    "OneClickMLOps.makeComponent(\n",
    "    name=\"create_dataset\",\n",
    "    description=\"Loads data from BQ and writes a dataframe as a csv to GCS.\", # optional\n",
    "    params=[\n",
    "        {\"name\": \"bq_table\", \"type\": str, \"description\": \"The bq input table.\"}, # descriptions are optional\n",
    "        {\"name\": \"data_path\", \"type\": str, \"description\": \"GS location where the training data is written.\"},\n",
    "        {\"name\": \"project_id\", \"type\": str, \"description\": \"Project_id.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "bq_client = bigquery.Client(project=project_id)\n",
    "\n",
    "def get_query(bq_input_table: str) -> str:\n",
    "    \"\"\"Generates BQ Query to read data.\n",
    "\n",
    "    Args:\n",
    "    bq_input_table: The full name of the bq input table to be read into\n",
    "    the dataframe (e.g. <project>.<dataset>.<table>)\n",
    "    Returns: A BQ query string.\n",
    "    \"\"\"\n",
    "    return f\"\"\"\n",
    "    SELECT *\n",
    "    FROM `{bq_input_table}`\n",
    "    \"\"\"\n",
    "\n",
    "def load_bq_data(query: str, client: bigquery.Client) -> pd.DataFrame:\n",
    "    \"\"\"Loads data from bq into a Pandas Dataframe for EDA.\n",
    "    Args:\n",
    "    query: BQ Query to generate data.\n",
    "    client: BQ Client used to execute query.\n",
    "    Returns:\n",
    "    pd.DataFrame: A dataframe with the requested data.\n",
    "    \"\"\"\n",
    "    df = client.query(query).to_dataframe()\n",
    "    return df\n",
    "\n",
    "dataframe = load_bq_data(get_query(bq_table), bq_client)\n",
    "dataframe.to_csv(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4476cb4-91c5-42ff-a500-8cc275fedbd1",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5371be73-db3f-4d79-bde7-94fcd5ea13b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%define\n",
    "OneClickMLOps.makeComponent(\n",
    "    name=\"train_model\",\n",
    "    description=\"Trains a decision tree on the training data.\",\n",
    "    params=[\n",
    "        {\"name\": \"model_directory\", \"type\": str, \"description\": \"GS location of saved model.\"},\n",
    "        {\"name\": \"data_path\", \"type\": str, \"description\": \"GS location where the training data.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "def save_model(model, model_directory):\n",
    "    \"\"\"Saves a model to uri.\"\"\"\n",
    "    filename = f'model.pkl'\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    \n",
    "    bucket_name = model_directory.split('/')[2]\n",
    "    prefix='/'.join(model_directory.split('/')[3:])\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(os.path.join(prefix, filename))\n",
    "    blob.upload_from_filename(filename)\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "labels = df.pop(\"Class\").tolist()\n",
    "data = df.values.tolist()\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels)\n",
    "skmodel = DecisionTreeClassifier()\n",
    "skmodel.fit(x_train,y_train)\n",
    "score = skmodel.score(x_test,y_test)\n",
    "print('accuracy is:',score)\n",
    "\n",
    "output_uri = os.path.join(model_directory, f'model.pkl')\n",
    "save_model(skmodel, model_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ed1eaa",
   "metadata": {},
   "source": [
    "## Uploading & Deploying the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47377d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%define\n",
    "OneClickMLOps.makeComponent(\n",
    "    name=\"deploy_model\",\n",
    "    description=\"Trains a decision tree on the training data.\",\n",
    "    params=[\n",
    "        {\"name\": \"model_directory\", \"type\": str, \"description\": \"GS location of saved model.\"},\n",
    "        {\"name\": \"project_id\", \"type\": str, \"description\": \"Project_id.\"},\n",
    "        {\"name\": \"region\", \"type\": str, \"description\": \"Region.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "aiplatform.init(project=project_id, location=region)\n",
    "deployed_model = aiplatform.Model.upload(\n",
    "    display_name=\"beans-model-pipeline\",\n",
    "    artifact_uri = model_directory,\n",
    "    serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-24:latest\"\n",
    ")\n",
    "endpoint = deployed_model.deploy(machine_type=\"n1-standard-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d96dcb-a020-4bab-b0e3-1e32f6b2aecf",
   "metadata": {},
   "source": [
    "## Define and Run the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3756b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_params = {\n",
    "    \"bq_table\": bq_table,\n",
    "    \"model_directory\": model_directory,\n",
    "    \"data_path\": data_path,\n",
    "    \"project_id\": project_id,\n",
    "    \"region\": region\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a36e0154",
   "metadata": {},
   "outputs": [],
   "source": [
    "OneClickMLOps.makePipeline(\n",
    "    name=\"training-pipeline\",\n",
    "    description=\"description\", # optional\n",
    "    params=[\n",
    "        {\"name\": \"bq_table\", \"type\": str, \"description\": \"Description.\"}, # descriptions are optional\n",
    "        {\"name\": \"model_directory\", \"type\": str, \"description\": \"Description.\"},\n",
    "        {\"name\": \"data_path\", \"type\": str, \"description\": \"Description.\"},\n",
    "        {\"name\": \"project_id\", \"type\": str, \"description\": \"Description.\"},\n",
    "        {\"name\": \"region\", \"type\": str, \"description\": \"Description.\"}\n",
    "    ],\n",
    "    pipeline=[{\n",
    "        \"component_name\": \"create_dataset\", \"param_mapping\": [\n",
    "            (\"bq_table\", \"bq_table\"), # (component_param, pipeline_param)\n",
    "            (\"data_path\", \"data_path\"),\n",
    "            (\"project_id\", \"project_id\")\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"component_name\": \"train_model\", \"param_mapping\": [\n",
    "            (\"model_directory\", \"model_directory\"),\n",
    "            (\"data_path\", \"data_path\")\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"component_name\": \"deploy_model\", \"param_mapping\": [\n",
    "            (\"model_directory\", \"model_directory\"),\n",
    "            (\"project_id\", \"project_id\"),\n",
    "            (\"region\", \"region\")\n",
    "        ]\n",
    "    }]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ef279e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;32m BUILDING COMPONENTS \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 28 file(s) totalling 65.3 KiB before compression.\n",
      "Some files were not included in the source upload.\n",
      "\n",
      "Check the gcloud log [/Users/srastatter/.config/gcloud/logs/2022.12.16/16.14.56.972550.log] to see which files and the contents of the\n",
      "default gcloudignore file used (see `$ gcloud topic gcloudignore` to learn\n",
      "more).\n",
      "\n",
      "Uploading tarball of [..] to [gs://sandbox-srastatter_cloudbuild/source/1671225296.999804-56e39c845845493da031a02e4840fb0b.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/sandbox-srastatter/locations/global/builds/85673889-b6e8-4e4e-bb8b-f15c9d62dbe7].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/85673889-b6e8-4e4e-bb8b-f15c9d62dbe7?project=1006819402307 ].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"85673889-b6e8-4e4e-bb8b-f15c9d62dbe7\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://sandbox-srastatter_cloudbuild/source/1671225296.999804-56e39c845845493da031a02e4840fb0b.tgz#1671225297378945\n",
      "Copying gs://sandbox-srastatter_cloudbuild/source/1671225296.999804-56e39c845845493da031a02e4840fb0b.tgz#1671225297378945...\n",
      "/ [1 files][ 17.6 KiB/ 17.6 KiB]                                                \n",
      "Operation completed over 1 objects/17.6 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  17.92kB\n",
      "Step 1/6 : FROM python:3.9\n",
      "3.9: Pulling from library/python\n",
      "f2f58072e9ed: Pulling fs layer\n",
      "5c8cfbf51e6e: Pulling fs layer\n",
      "aa3a609d1579: Pulling fs layer\n",
      "094e7d9bb04e: Pulling fs layer\n",
      "2cbfd734f382: Pulling fs layer\n",
      "aa86ac293d0f: Pulling fs layer\n",
      "ea442e3d4174: Pulling fs layer\n",
      "c662908b49d7: Pulling fs layer\n",
      "f7e80cce9b62: Pulling fs layer\n",
      "094e7d9bb04e: Waiting\n",
      "2cbfd734f382: Waiting\n",
      "aa86ac293d0f: Waiting\n",
      "ea442e3d4174: Waiting\n",
      "c662908b49d7: Waiting\n",
      "f7e80cce9b62: Waiting\n",
      "5c8cfbf51e6e: Download complete\n",
      "aa3a609d1579: Verifying Checksum\n",
      "aa3a609d1579: Download complete\n",
      "f2f58072e9ed: Verifying Checksum\n",
      "f2f58072e9ed: Download complete\n",
      "094e7d9bb04e: Verifying Checksum\n",
      "094e7d9bb04e: Download complete\n",
      "aa86ac293d0f: Verifying Checksum\n",
      "aa86ac293d0f: Download complete\n",
      "c662908b49d7: Verifying Checksum\n",
      "c662908b49d7: Download complete\n",
      "f7e80cce9b62: Verifying Checksum\n",
      "f7e80cce9b62: Download complete\n",
      "ea442e3d4174: Verifying Checksum\n",
      "ea442e3d4174: Download complete\n",
      "2cbfd734f382: Verifying Checksum\n",
      "2cbfd734f382: Download complete\n",
      "f2f58072e9ed: Pull complete\n",
      "5c8cfbf51e6e: Pull complete\n",
      "aa3a609d1579: Pull complete\n",
      "094e7d9bb04e: Pull complete\n",
      "2cbfd734f382: Pull complete\n",
      "aa86ac293d0f: Pull complete\n",
      "ea442e3d4174: Pull complete\n",
      "c662908b49d7: Pull complete\n",
      "f7e80cce9b62: Pull complete\n",
      "Digest: sha256:929da7c4e1285e844e70e901267059f63ea958778695a867111a77eaf09700ff\n",
      "Status: Downloaded newer image for python:3.9\n",
      " ---> 7d357ce6a803\n",
      "Step 2/6 : RUN python -m pip install --upgrade pip\n",
      " ---> Running in 99371caa6e50\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.9/site-packages (22.0.4)\n",
      "Collecting pip\n",
      "  Downloading pip-22.3.1-py3-none-any.whl (2.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 27.8 MB/s eta 0:00:00\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.0.4\n",
      "    Uninstalling pip-22.0.4:\n",
      "      Successfully uninstalled pip-22.0.4\n",
      "Successfully installed pip-22.3.1\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 99371caa6e50\n",
      " ---> 150f02ce7b79\n",
      "Step 3/6 : COPY requirements.txt .\n",
      " ---> af3cdd89702c\n",
      "Step 4/6 : RUN python -m pip install -r     requirements.txt --quiet --no-cache-dir     && rm -f requirements.txt\n",
      " ---> Running in 4d0f2f15cae6\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 4d0f2f15cae6\n",
      " ---> 6a650133d202\n",
      "Step 5/6 : COPY ./src /pipelines/component/src\n",
      " ---> 1fca6421b3ce\n",
      "Step 6/6 : ENTRYPOINT [\"/bin/bash\"]\n",
      " ---> Running in 618e9088f1ad\n",
      "Removing intermediate container 618e9088f1ad\n",
      " ---> 24211c0a9435\n",
      "Successfully built 24211c0a9435\n",
      "Successfully tagged us-central1-docker.pkg.dev/sandbox-srastatter/vertex-mlops-af/components/component_base:latest\n",
      "PUSH\n",
      "Pushing us-central1-docker.pkg.dev/sandbox-srastatter/vertex-mlops-af/components/component_base:latest\n",
      "The push refers to repository [us-central1-docker.pkg.dev/sandbox-srastatter/vertex-mlops-af/components/component_base]\n",
      "134e77664f8c: Preparing\n",
      "5f162ac9dcce: Preparing\n",
      "6a92be46040f: Preparing\n",
      "2a8ba3a48d33: Preparing\n",
      "f95cd016d6e3: Preparing\n",
      "79b4fe16a228: Preparing\n",
      "792cd6061bec: Preparing\n",
      "1cad4dc57058: Preparing\n",
      "4ff8844d474a: Preparing\n",
      "b77487480ddb: Preparing\n",
      "cd247c0fb37b: Preparing\n",
      "cfdd5c3bd77e: Preparing\n",
      "870a241bfebd: Preparing\n",
      "792cd6061bec: Waiting\n",
      "1cad4dc57058: Waiting\n",
      "4ff8844d474a: Waiting\n",
      "b77487480ddb: Waiting\n",
      "cd247c0fb37b: Waiting\n",
      "cfdd5c3bd77e: Waiting\n",
      "870a241bfebd: Waiting\n",
      "79b4fe16a228: Waiting\n",
      "f95cd016d6e3: Layer already exists\n",
      "79b4fe16a228: Layer already exists\n",
      "792cd6061bec: Layer already exists\n",
      "6a92be46040f: Pushed\n",
      "1cad4dc57058: Layer already exists\n",
      "4ff8844d474a: Layer already exists\n",
      "134e77664f8c: Pushed\n",
      "b77487480ddb: Layer already exists\n",
      "cfdd5c3bd77e: Layer already exists\n",
      "cd247c0fb37b: Layer already exists\n",
      "870a241bfebd: Layer already exists\n",
      "2a8ba3a48d33: Pushed\n",
      "5f162ac9dcce: Pushed\n",
      "latest: digest: sha256:2a7151e87d4bc96f8b7756e1175909d140b8d239509f3129c909efc010249f11 size: 3057\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                            IMAGES                                                                                             STATUS\n",
      "85673889-b6e8-4e4e-bb8b-f15c9d62dbe7  2022-12-16T21:14:57+00:00  2M21S     gs://sandbox-srastatter_cloudbuild/source/1671225296.999804-56e39c845845493da031a02e4840fb0b.tgz  us-central1-docker.pkg.dev/sandbox-srastatter/vertex-mlops-af/components/component_base (+1 more)  SUCCESS\n",
      "\u001b[0;32m BUILDING PIPELINE SPEC \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srastatter/Library/Python/3.9/lib/python/site-packages/kfp/v2/compiler/compiler.py:1290: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;32m RUNNING PIPELINE JOB \u001b[0m\n",
      "Creating PipelineJob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob created. Resource name: projects/1006819402307/locations/us-central1/pipelineJobs/training-pipeline-20221216161726\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/1006819402307/locations/us-central1/pipelineJobs/training-pipeline-20221216161726')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/training-pipeline-20221216161726?project=1006819402307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/1006819402307/locations/us-central1/pipelineJobs/training-pipeline-20221216161726\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/1006819402307/locations/us-central1/pipelineJobs/training-pipeline-20221216161726')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/training-pipeline-20221216161726?project=1006819402307\n"
     ]
    }
   ],
   "source": [
    "OneClickMLOps.go(project_id='sandbox-srastatter', pipeline_params=pipeline_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dd423f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
