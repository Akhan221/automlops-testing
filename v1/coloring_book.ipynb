{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f938540",
   "metadata": {},
   "source": [
    "# MLOps Pipeline Coloring Book\n",
    "\n",
    "This notebook may be used to construct a proto-type pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9222df33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 1.8.17\n"
     ]
    }
   ],
   "source": [
    "# Check that package is installed correctly. The KFP SDK version should be >=1.6:\n",
    "!python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bfeea8",
   "metadata": {},
   "source": [
    "TODO: Add docstrings\n",
    "\n",
    "Terraform (https://github.com/GoogleCloudPlatform/vertex-pipelines-end-to-end-samples/tree/main/terraform):\n",
    "- Create artifact registry\n",
    "- Create GS bucket\n",
    "- Create service account for running pipeline\n",
    "- IAM Privileges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2219ee67",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "164ff1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.dsl import pipeline, component, Artifact, Dataset, Input, Metrics, Model, Output, InputPath, OutputPath\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import aiplatform_v1\n",
    "import datetime\n",
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "from utils import MLOpsBoxer\n",
    "\n",
    "@register_cell_magic\n",
    "def execute_and_save(file, cell):\n",
    "    'Run and save python code block to a file'\n",
    "    with open(file, 'wt') as fd:\n",
    "        fd.write(cell)\n",
    "    code = compile(cell, file, 'exec')\n",
    "    exec(code, globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac0c712-acdd-4830-a9bd-44f41eaa590c",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94a96074-761f-40f0-84bb-adfcb9bf6911",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"google-cloud-bigquery\", \n",
    "        \"pandas\",\n",
    "        \"pyarrow\",\n",
    "        \"db_dtypes\"\n",
    "    ],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"create_dataset.yaml\"\n",
    ")\n",
    "def create_dataset(\n",
    "    bq_table: str,\n",
    "    output_data_path: OutputPath(\"Dataset\"),\n",
    "    project: str\n",
    "):\n",
    "    from google.cloud import bigquery\n",
    "    import pandas as pd\n",
    "    bq_client = bigquery.Client(project=project)\n",
    "\n",
    "\n",
    "    def get_query(bq_input_table: str) -> str:\n",
    "        \"\"\"Generates BQ Query to read data.\n",
    "\n",
    "        Args:\n",
    "        bq_input_table: The full name of the bq input table to be read into\n",
    "        the dataframe (e.g. <project>.<dataset>.<table>)\n",
    "        Returns: A BQ query string.\n",
    "        \"\"\"\n",
    "        return f\"\"\"\n",
    "        SELECT *\n",
    "        FROM `{bq_input_table}`\n",
    "        \"\"\"\n",
    "\n",
    "    def load_bq_data(query: str, client: bigquery.Client) -> pd.DataFrame:\n",
    "        \"\"\"Loads data from bq into a Pandas Dataframe for EDA.\n",
    "        Args:\n",
    "        query: BQ Query to generate data.\n",
    "        client: BQ Client used to execute query.\n",
    "        Returns:\n",
    "        pd.DataFrame: A dataframe with the requested data.\n",
    "        \"\"\"\n",
    "        df = client.query(query).to_dataframe()\n",
    "        return df\n",
    "\n",
    "    dataframe = load_bq_data(get_query(bq_table), bq_client)\n",
    "    dataframe.to_csv(output_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4476cb4-91c5-42ff-a500-8cc275fedbd1",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5371be73-db3f-4d79-bde7-94fcd5ea13b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"scikit-learn\",\n",
    "        \"pandas\",\n",
    "        \"joblib\",\n",
    "        \"tensorflow\"\n",
    "    ],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"train_model.yaml\",\n",
    ")\n",
    "def train_model(\n",
    "    output_model_directory: str,\n",
    "    dataset: Input[Dataset],\n",
    "    metrics: Output[Metrics],\n",
    "    model: Output[Model]\n",
    "):\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from joblib import dump\n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "    import pickle\n",
    "    import os\n",
    "    \n",
    "    def save_model(model, uri):\n",
    "        \"\"\"Saves a model to uri.\"\"\"\n",
    "        with tf.io.gfile.GFile(uri, 'w') as f:\n",
    "            pickle.dump(model, f)\n",
    "    \n",
    "    df = pd.read_csv(dataset.path)\n",
    "    labels = df.pop(\"Class\").tolist()\n",
    "    data = df.values.tolist()\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, labels)\n",
    "    skmodel = DecisionTreeClassifier()\n",
    "    skmodel.fit(x_train,y_train)\n",
    "    score = skmodel.score(x_test,y_test)\n",
    "    print('accuracy is:',score)\n",
    "    metrics.log_metric(\"accuracy\",(score * 100.0))\n",
    "    metrics.log_metric(\"framework\", \"Scikit Learn\")\n",
    "    metrics.log_metric(\"dataset_size\", len(df))\n",
    "\n",
    "    output_uri = os.path.join(output_model_directory, f'model.pkl')\n",
    "    save_model(skmodel, output_uri)\n",
    "    model.path = output_model_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ed1eaa",
   "metadata": {},
   "source": [
    "## Uploading & Deploying the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47377d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"google-cloud-aiplatform\"\n",
    "    ],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"deploy_model.yaml\",\n",
    ")\n",
    "def deploy_model(\n",
    "    model: Input[Model],\n",
    "    project: str,\n",
    "    region: str,\n",
    "    vertex_endpoint: Output[Artifact],\n",
    "    vertex_model: Output[Model]\n",
    "):\n",
    "    from google.cloud import aiplatform\n",
    "    aiplatform.init(project=project, location=region)\n",
    "    deployed_model = aiplatform.Model.upload(\n",
    "        display_name=\"beans-model-pipeline\",\n",
    "        artifact_uri = model.uri,\n",
    "        serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-24:latest\"\n",
    "    )\n",
    "    endpoint = deployed_model.deploy(machine_type=\"n1-standard-4\")\n",
    "    vertex_endpoint.uri = endpoint.resource_name\n",
    "    vertex_model.uri = deployed_model.resource_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d96dcb-a020-4bab-b0e3-1e32f6b2aecf",
   "metadata": {},
   "source": [
    "## Define and Run the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a36e0154",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%execute_and_save pipeline.py \n",
    "@dsl.pipeline(name='training-pipeline')\n",
    "def pipeline(bq_table: str,\n",
    "             output_model_directory: str,\n",
    "             project: str,\n",
    "             region: str,\n",
    "            ):\n",
    "    \n",
    "    dataset_task = create_dataset(\n",
    "        bq_table=bq_table, \n",
    "        project=project)\n",
    "\n",
    "    model_task = train_model(\n",
    "        output_model_directory=output_model_directory,\n",
    "        dataset=dataset_task.output)\n",
    "\n",
    "    deploy_task = deploy_model(\n",
    "        model=model_task.outputs[\"model\"],\n",
    "        project=project,\n",
    "        region=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e5b59f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_params = {\n",
    "    \"bq_table\": \"sandbox-srastatter.mlops_boxer_test.dry-beans\",\n",
    "    \"output_model_directory\": f\"gs://mlops-boxer-test/trained_models/{datetime.datetime.now()}\",\n",
    "    \"project\": \"sandbox-srastatter\",\n",
    "    \"region\": \"us-central1\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef279e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;32m BUILDING COMPONENTS \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Listing items under project sandbox-srastatter, location us-central1.\n",
      "\n",
      "Creating temporary tarball archive of 38 file(s) totalling 97.8 KiB before compression.\n",
      "Uploading tarball of [..] to [gs://sandbox-srastatter_cloudbuild/source/1671221152.024353-21c61d2f0682413985a4fc149097865a.tgz]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vertex-mlops-af                DOCKER  STANDARD_REPOSITORY               us-central1          Google-managed key  2022-05-04T20:27:05  2022-12-16T19:06:22  575.269\n",
      "Artifact Registry: vertex-mlops-af already exists in project sandbox-srastatter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created [https://cloudbuild.googleapis.com/v1/projects/sandbox-srastatter/locations/global/builds/1dbc3dc2-919d-4943-839a-76cf082272a1].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/1dbc3dc2-919d-4943-839a-76cf082272a1?project=1006819402307 ].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://sandbox-srastatter-bucket/\n",
      "GS Bucket: sandbox-srastatter-bucket already exists in project sandbox-srastatter\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"1dbc3dc2-919d-4943-839a-76cf082272a1\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://sandbox-srastatter_cloudbuild/source/1671221152.024353-21c61d2f0682413985a4fc149097865a.tgz#1671221152418782\n",
      "Copying gs://sandbox-srastatter_cloudbuild/source/1671221152.024353-21c61d2f0682413985a4fc149097865a.tgz#1671221152418782...\n",
      "/ [1 files][ 26.2 KiB/ 26.2 KiB]                                                \n",
      "Operation completed over 1 objects/26.2 KiB.\n",
      "BUILD\n",
      "Starting Step #0 - \"Build component: train_model\"\n",
      "Step #0 - \"Build component: train_model\": Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Step #0 - \"Build component: train_model\": Sending build context to Docker daemon  7.168kB\n",
      "Step #0 - \"Build component: train_model\": Step 1/7 : FROM python:3.9\n",
      "Step #0 - \"Build component: train_model\": 3.9: Pulling from library/python\n",
      "Step #0 - \"Build component: train_model\": f2f58072e9ed: Pulling fs layer\n",
      "Step #0 - \"Build component: train_model\": 5c8cfbf51e6e: Pulling fs layer\n",
      "Step #0 - \"Build component: train_model\": aa3a609d1579: Pulling fs layer\n",
      "Step #0 - \"Build component: train_model\": 094e7d9bb04e: Pulling fs layer\n",
      "Step #0 - \"Build component: train_model\": 2cbfd734f382: Pulling fs layer\n",
      "Step #0 - \"Build component: train_model\": aa86ac293d0f: Pulling fs layer\n",
      "Step #0 - \"Build component: train_model\": ea442e3d4174: Pulling fs layer\n",
      "Step #0 - \"Build component: train_model\": c662908b49d7: Pulling fs layer\n",
      "Step #0 - \"Build component: train_model\": f7e80cce9b62: Pulling fs layer\n",
      "Step #0 - \"Build component: train_model\": 094e7d9bb04e: Waiting\n",
      "Step #0 - \"Build component: train_model\": 2cbfd734f382: Waiting\n",
      "Step #0 - \"Build component: train_model\": aa86ac293d0f: Waiting\n",
      "Step #0 - \"Build component: train_model\": ea442e3d4174: Waiting\n",
      "Step #0 - \"Build component: train_model\": c662908b49d7: Waiting\n",
      "Step #0 - \"Build component: train_model\": f7e80cce9b62: Waiting\n",
      "Step #0 - \"Build component: train_model\": 5c8cfbf51e6e: Verifying Checksum\n",
      "Step #0 - \"Build component: train_model\": 5c8cfbf51e6e: Download complete\n",
      "Step #0 - \"Build component: train_model\": aa3a609d1579: Verifying Checksum\n",
      "Step #0 - \"Build component: train_model\": aa3a609d1579: Download complete\n",
      "Step #0 - \"Build component: train_model\": f2f58072e9ed: Verifying Checksum\n",
      "Step #0 - \"Build component: train_model\": f2f58072e9ed: Download complete\n",
      "Step #0 - \"Build component: train_model\": aa86ac293d0f: Verifying Checksum\n",
      "Step #0 - \"Build component: train_model\": aa86ac293d0f: Download complete\n",
      "Step #0 - \"Build component: train_model\": 094e7d9bb04e: Verifying Checksum\n",
      "Step #0 - \"Build component: train_model\": 094e7d9bb04e: Download complete\n",
      "Step #0 - \"Build component: train_model\": c662908b49d7: Verifying Checksum\n",
      "Step #0 - \"Build component: train_model\": c662908b49d7: Download complete\n",
      "Step #0 - \"Build component: train_model\": ea442e3d4174: Verifying Checksum\n",
      "Step #0 - \"Build component: train_model\": ea442e3d4174: Download complete\n",
      "Step #0 - \"Build component: train_model\": f7e80cce9b62: Verifying Checksum\n",
      "Step #0 - \"Build component: train_model\": f7e80cce9b62: Download complete\n",
      "Step #0 - \"Build component: train_model\": 2cbfd734f382: Verifying Checksum\n",
      "Step #0 - \"Build component: train_model\": 2cbfd734f382: Download complete\n",
      "Step #0 - \"Build component: train_model\": f2f58072e9ed: Pull complete\n",
      "Step #0 - \"Build component: train_model\": 5c8cfbf51e6e: Pull complete\n",
      "Step #0 - \"Build component: train_model\": aa3a609d1579: Pull complete\n",
      "Step #0 - \"Build component: train_model\": 094e7d9bb04e: Pull complete\n",
      "Step #0 - \"Build component: train_model\": 2cbfd734f382: Pull complete\n",
      "Step #0 - \"Build component: train_model\": aa86ac293d0f: Pull complete\n",
      "Step #0 - \"Build component: train_model\": ea442e3d4174: Pull complete\n",
      "Step #0 - \"Build component: train_model\": c662908b49d7: Pull complete\n",
      "Step #0 - \"Build component: train_model\": f7e80cce9b62: Pull complete\n",
      "Step #0 - \"Build component: train_model\": Digest: sha256:929da7c4e1285e844e70e901267059f63ea958778695a867111a77eaf09700ff\n",
      "Step #0 - \"Build component: train_model\": Status: Downloaded newer image for python:3.9\n",
      "Step #0 - \"Build component: train_model\":  ---> 7d357ce6a803\n",
      "Step #0 - \"Build component: train_model\": Step 2/7 : RUN python -m pip install --upgrade pip\n",
      "Step #0 - \"Build component: train_model\":  ---> Running in 823eae5581b6\n",
      "Step #0 - \"Build component: train_model\": Requirement already satisfied: pip in /usr/local/lib/python3.9/site-packages (22.0.4)\n",
      "Step #0 - \"Build component: train_model\": Collecting pip\n",
      "Step #0 - \"Build component: train_model\":   Downloading pip-22.3.1-py3-none-any.whl (2.1 MB)\n",
      "Step #0 - \"Build component: train_model\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 57.4 MB/s eta 0:00:00\n",
      "Step #0 - \"Build component: train_model\": Installing collected packages: pip\n",
      "Step #0 - \"Build component: train_model\":   Attempting uninstall: pip\n",
      "Step #0 - \"Build component: train_model\":     Found existing installation: pip 22.0.4\n",
      "Step #0 - \"Build component: train_model\":     Uninstalling pip-22.0.4:\n",
      "Step #0 - \"Build component: train_model\":       Successfully uninstalled pip-22.0.4\n",
      "Step #0 - \"Build component: train_model\": Successfully installed pip-22.3.1\n",
      "Step #0 - \"Build component: train_model\": \u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "Step #0 - \"Build component: train_model\": \u001b[0mRemoving intermediate container 823eae5581b6\n",
      "Step #0 - \"Build component: train_model\":  ---> 225caa053982\n",
      "Step #0 - \"Build component: train_model\": Step 3/7 : COPY requirements.txt .\n",
      "Step #0 - \"Build component: train_model\":  ---> b4255a905a30\n",
      "Step #0 - \"Build component: train_model\": Step 4/7 : RUN python -m pip install -r     requirements.txt --quiet --no-cache-dir     && rm -f requirements.txt\n",
      "Step #0 - \"Build component: train_model\":  ---> Running in 7ac67256cd21\n",
      "Step #0 - \"Build component: train_model\": \u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "Step #0 - \"Build component: train_model\": \u001b[0mRemoving intermediate container 7ac67256cd21\n",
      "Step #0 - \"Build component: train_model\":  ---> b42511110aee\n",
      "Step #0 - \"Build component: train_model\": Step 5/7 : WORKDIR /\n",
      "Step #0 - \"Build component: train_model\":  ---> Running in 5974b190ea2e\n",
      "Step #0 - \"Build component: train_model\": Removing intermediate container 5974b190ea2e\n",
      "Step #0 - \"Build component: train_model\":  ---> 4d2f7f08102d\n",
      "Step #0 - \"Build component: train_model\": Step 6/7 : COPY trainer /trainer\n",
      "Step #0 - \"Build component: train_model\":  ---> da1606b7a3f2\n",
      "Step #0 - \"Build component: train_model\": Step 7/7 : ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]\n",
      "Step #0 - \"Build component: train_model\":  ---> Running in 8924410f5562\n",
      "Step #0 - \"Build component: train_model\": Removing intermediate container 8924410f5562\n",
      "Step #0 - \"Build component: train_model\":  ---> f3999e82b232\n",
      "Step #0 - \"Build component: train_model\": Successfully built f3999e82b232\n",
      "Step #0 - \"Build component: train_model\": Successfully tagged us-central1-docker.pkg.dev/sandbox-srastatter/vertex-mlops-af/components/train_model:latest\n",
      "Finished Step #0 - \"Build component: train_model\"\n",
      "Starting Step #1 - \"Build component: create_dataset\"\n",
      "Step #1 - \"Build component: create_dataset\": Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Step #1 - \"Build component: create_dataset\": Sending build context to Docker daemon  10.24kB\n"
     ]
    }
   ],
   "source": [
    "MLOpsBoxer.go(project_id='sandbox-srastatter',\n",
    "              pipeline_params=pipeline_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e2f07ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Listing items under project sandbox-srastatter, location us-central1.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlops-boxer-test               DOCKER  STANDARD_REPOSITORY               us-central1          Google-managed key  2022-12-12T18:44:42  2022-12-13T14:37:22  2992.602\n",
      "Artifact Registry: mlops-boxer-test already exists in project sandbox-srastatter\n",
      "gs://mlops-boxer-test/\n",
      "GS Bucket: mlops-boxer-test already exists in project sandbox-srastatter\n"
     ]
    }
   ],
   "source": [
    "MLOpsBoxer.generate(project_id='sandbox-srastatter',\n",
    "                    af_registry_name='mlops-boxer-test',\n",
    "                    af_registry_location='us-central1',\n",
    "                    gs_bucket_location='us-central1',\n",
    "                    gs_bucket_name='mlops-boxer-test',\n",
    "                    pipeline_params=pipeline_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01290696-afa4-42c0-9649-e4db8d52adfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff5fba56",
   "metadata": {},
   "source": [
    "### Compile and run the end-to-end ML pipeline\n",
    "With our full pipeline defined, it's time to compile it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "794c18bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srastatter/Library/Python/3.9/lib/python/site-packages/kfp/v2/compiler/compiler.py:1290: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, \n",
    "    package_path=\"training_pipeline_job.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951f42d8",
   "metadata": {},
   "source": [
    "Next, kick off a pipeline run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "867e369f-a635-4689-9ad0-6f8ea095b597",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_params = {\n",
    "    \"bq_table\": \"test\",\n",
    "    \"output_model_directory\": \"test\",\n",
    "    \"output_data_path\": \"test\",\n",
    "    \"project\": \"test\",\n",
    "    \"region\": \"test\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb3f5dd5-bcd8-42d5-9d12-821246077267",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/512807375329/locations/us-central1/pipelineJobs/embeddings-pipeline-20220520195220\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/512807375329/locations/us-central1/pipelineJobs/embeddings-pipeline-20220520195220')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/embeddings-pipeline-20220520195220?project=512807375329\n"
     ]
    }
   ],
   "source": [
    "aiplatform.init(project=PROJECT_ID)\n",
    "training_job = aiplatform.PipelineJob(\n",
    "    display_name = \"mlops-sp2-init-pipeline-run\",\n",
    "    template_path = \"training_pipeline_job.json\",\n",
    "    pipeline_root = PIPELINE_ROOT,\n",
    "    parameter_values = pipeline_params,\n",
    "    enable_caching = False\n",
    ")\n",
    "\n",
    "training_job.submit(service_account='gia-vertex-ai-custom@poc-gia-2034083092.iam.gserviceaccount.com')\n",
    "#training_job.run()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m91"
  },
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
