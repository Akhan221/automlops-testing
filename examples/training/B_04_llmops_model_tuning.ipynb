{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b45b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8790d7ed",
   "metadata": {},
   "source": [
    "# AutoMLOps - Tuning and deploying a foundation model \n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/automlops/blob/main/example/automlops_example_notebook.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/automlops/blob/main/example/automlops_example_notebook.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/automlops/main/example/automlops_example_notebook.ipynb\">\n",
    "        <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f938540",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This tutorial explores using AutoMLOps for tuning a foundation LLM model. Creating an LLM requires massive amounts of data, significant computing resources, and specialized skills. On Vertex AI, tuning allows you to customize a foundation model for more specific tasks or knowledge domains.\n",
    "\n",
    "While the prompt design is excellent for quick experimentation, if training data is available, you can achieve higher quality by tuning the model. Tuning a model enables you to customize the model response based on examples of the task you want the model to perform.\n",
    "\n",
    "This tutorial is adapted from the [getting started tuning](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/examples/tuning/getting_started_tuning.ipynb) example for Vertex AI. For more details on tuning have a look at the [official documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22881a6a",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "In this tutorial, you learn how to fine tune an LLM in Vertex AI. You will then learn how to create and run MLOps pipelines integrated with CI/CD. The pipeline goes through the following steps:\n",
    "\n",
    "1. create_datasets: Custom component that queries Bigquery and saves data as jsonl format to GCS.\n",
    "2. tune_model: Custom component that prompt-tunes a foundation model.\n",
    "4. evaluate_model: Evaluate the tuned model.\n",
    "5. predict: Custom component that runs predictions with the tuned model.\n",
    "\n",
    "# Prerequisites\n",
    "\n",
    "In order to use AutoMLOps, the following are required:\n",
    "\n",
    "- Python 3.0 - 3.10\n",
    "- [Google Cloud SDK 407.0.0](https://cloud.google.com/sdk/gcloud/reference)\n",
    "- [beta 2022.10.21](https://cloud.google.com/sdk/gcloud/reference/beta)\n",
    "- `git` installed\n",
    "- `git` logged-in:\n",
    "```\n",
    "  git config --global user.email \"you@example.com\"\n",
    "  git config --global user.name \"Your Name\"\n",
    "```\n",
    "- [Application Default Credentials (ADC)](https://cloud.google.com/docs/authentication/provide-credentials-adc) are setup. This can be done through the following commands:\n",
    "```\n",
    "gcloud auth application-default login\n",
    "gcloud config set account <account@example.com>\n",
    "```\n",
    "\n",
    "# Dependencies\n",
    "- `docopt==0.6.2`,\n",
    "- `docstring-parser==0.15`,\n",
    "- `pipreqs==0.4.11`,\n",
    "- `PyYAML==5.4.1`,\n",
    "- `yarg==0.1.9`\n",
    "\n",
    "# APIs & IAM\n",
    "AutoMLOps will enable the following APIs:\n",
    "- [cloudresourcemanager.googleapis.com](https://cloud.google.com/resource-manager/reference/rest)\n",
    "- [aiplatform.googleapis.com](https://cloud.google.com/vertex-ai/docs/reference/rest)\n",
    "- [artifactregistry.googleapis.com](https://cloud.google.com/artifact-registry/docs/reference/rest)\n",
    "- [cloudbuild.googleapis.com](https://cloud.google.com/build/docs/api/reference/rest)\n",
    "- [cloudscheduler.googleapis.com](https://cloud.google.com/scheduler/docs/reference/rest)\n",
    "- [cloudtasks.googleapis.com](https://cloud.google.com/tasks/docs/reference/rest)\n",
    "- [compute.googleapis.com](https://cloud.google.com/compute/docs/reference/rest/v1)\n",
    "- [iam.googleapis.com](https://cloud.google.com/iam/docs/reference/rest)\n",
    "- [iamcredentials.googleapis.com](https://cloud.google.com/iam/docs/reference/credentials/rest)\n",
    "- [ml.googleapis.com](https://cloud.google.com/ai-platform/training/docs/reference/rest)\n",
    "- [run.googleapis.com](https://cloud.google.com/run/docs/reference/rest)\n",
    "- [storage.googleapis.com](https://cloud.google.com/storage/docs/apis)\n",
    "- [sourcerepo.googleapis.com](https://cloud.google.com/source-repositories/docs/reference/rest)\n",
    "\n",
    "AutoMLOps will update [IAM privileges](https://cloud.google.com/iam/docs/understanding-roles) for the following accounts:\n",
    "1. Pipeline Runner Service Account (one is created if it does exist, defaults to: vertex-pipelines@PROJECT_ID.iam.gserviceaccount.com). Roles added:\n",
    "- roles/aiplatform.user\n",
    "- roles/artifactregistry.reader\n",
    "- roles/bigquery.user\n",
    "- roles/bigquery.dataEditor\n",
    "- roles/iam.serviceAccountUser\n",
    "- roles/storage.admin\n",
    "- roles/run.admin\n",
    "2. Cloudbuild Default Service Account (PROJECT_NUMBER@cloudbuild.gserviceaccount.com). Roles added:\n",
    "- roles/run.admin\n",
    "- roles/iam.serviceAccountUser\n",
    "- roles/cloudtasks.enqueuer\n",
    "- roles/cloudscheduler.admin\n",
    "\n",
    "# User Guide\n",
    "\n",
    "For a user-guide, please view these [slides](../AutoMLOps_Implementation_Guide_External.pdf).\n",
    "\n",
    "# Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "- Vertex AI\n",
    "- BigQuery\n",
    "- Artifact Registry\n",
    "- Cloud Storage\n",
    "- Cloud Source Repository\n",
    "- Cloud Build\n",
    "- Cloud Run\n",
    "- Cloud Scheduler\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage.\n",
    "\n",
    "# Ground-rules for using AutoMLOps\n",
    "1. Do not use variables, functions, code, etc. not defined within the scope of a custom component. These custom components will become containers and will have no reference to the out of scope code.\n",
    "2. Import statements and helper functions must be added inside the function. Provide parameter type hints.\n",
    "3. Test each of your components for accuracy and correctness before running them using AutoMLOps. We cannot fix bugs automatically; bugs are much more difficult to fix once they are made into pipelines.\n",
    "4. If you are using Kubeflow, be sure to define all the requirements needed to run the custom component - it can be easy to leave out packages which will cause the container to fail when running within a pipeline. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04a20b9",
   "metadata": {},
   "source": [
    "### Quota\n",
    "**important**: Tuning the text-bison@001  model uses the tpu-v3-8 training resources and the accompanying quotas from your Google Cloud project. Each project has a default quota of eight v3-8 cores, which allows for one to two concurrent tuning jobs. If you want to run more concurrent jobs you need to request additional quota via the [Quotas page](https://console.cloud.google.com/iam-admin/quotas)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12381413",
   "metadata": {},
   "source": [
    "## Tuned Dataset\n",
    "\n",
    "Your model tuning dataset must be in a JSONL format where each line contains a single training example. You must make sure that you include instructions.\n",
    "\n",
    "You will use the StackOverflow data on [BigQuery public datasets](https://cloud.google.com/bigquery/public-data), limiting to questions with the `python` tag, and accepted answers for answers since 2020-01-01."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a231b629",
   "metadata": {},
   "source": [
    "## Setup Git\n",
    "Set up your git configuration below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f90b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global user.email 'you@example.com'\n",
    "!git config --global user.name 'Your Name'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e4d190",
   "metadata": {},
   "source": [
    "## Install AutoMLOps\n",
    "\n",
    "Install AutoMLOps from [PyPI](https://pypi.org/project/google-cloud-automlops/), or locally by cloning the repo and running `pip install .`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94451868",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install google-cloud-automlops --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17db55d5",
   "metadata": {},
   "source": [
    "## Restart the kernel\n",
    "Once you've installed the AutoMLOps package, you need to restart the notebook kernel so it can find the package.\n",
    "\n",
    "**Note: Once this cell has finished running, continue on. You do not need to re-run any of the cells above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57c53b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv('IS_TESTING'):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250d511b",
   "metadata": {},
   "source": [
    "## Set your project ID\n",
    "Set your project ID below. If you don't know your project ID, leave the field blank and the following cells may be able to find it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "931ff517",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = '[your-project-id]'  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be0be295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID: automlops-sandbox\n"
     ]
    }
   ],
   "source": [
    "if PROJECT_ID == '' or PROJECT_ID is None or PROJECT_ID == '[your-project-id]':\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print('Project ID:', PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c36482d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\r\n"
     ]
    }
   ],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460acdf5-2fb2-4cce-ac3f-93dce3efd907",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Automating pipeline creation with AutoMLOps<a id='automlops'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e69055-9e3d-4269-87dd-1825e0004d7d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import AutoMLOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a240118d-6e9d-494a-a0e7-83d67360c286",
   "metadata": {},
   "outputs": [],
   "source": [
    "from AutoMLOps import AutoMLOps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ad9f17-874f-4845-81ba-d2419d3cf5e4",
   "metadata": {},
   "source": [
    "## Other Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1041dd30-240b-4cd8-b66f-8eba8f8f9724",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install kfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb7e69f1-6129-44cd-91b5-5c840f403334",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import Artifact, Dataset, Metrics, Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5061a5-f94e-4285-b98f-d663b271e6b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Clear the cache\n",
    "`AutoMLOps.clear_cache` will remove previous instantiations of AutoMLOps components and pipelines. Use this function if you have previously defined a component that you no longer need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5d1d275-db30-4189-ba54-41ea700e4b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache cleared.\n"
     ]
    }
   ],
   "source": [
    "AutoMLOps.clear_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fc9f06-de7c-4593-afe6-9a359152cc8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define an AutoMLOps component for Creating Train & Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a302583-85a8-4687-bd9a-578f8c955eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@AutoMLOps.component\n",
    "def create_datasets(\n",
    "    lookback_date: str,\n",
    "    project_id: str,\n",
    "    test_data_path: str,\n",
    "    train_data_path: str\n",
    "):\n",
    "    \"\"\"Custom component that prepares the stackoverflow Questions and Answers.\n",
    "\n",
    "    Args:\n",
    "        lookback_date: The start date for posts.\n",
    "        project_id: The project ID.\n",
    "        test_data_path: The gcs location to write the jsonl for evaluation.\n",
    "        train_data_path: The gcs location to write the jsonl for training.\n",
    "    \"\"\"    \n",
    "    import pandas as pd\n",
    "    from google.cloud import bigquery\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    bq_client = bigquery.Client(project=project_id)\n",
    "\n",
    "    def get_query() -> str:\n",
    "        \"\"\"Generates BQ Query to read data.\"\"\"\n",
    "        \n",
    "        return f'''SELECT\n",
    "        CONCAT(q.title, q.body) as input_text,\n",
    "        a.body AS output_text\n",
    "        FROM\n",
    "            `bigquery-public-data.stackoverflow.posts_questions` q\n",
    "        JOIN\n",
    "            `bigquery-public-data.stackoverflow.posts_answers` a\n",
    "        ON\n",
    "            q.accepted_answer_id = a.id\n",
    "        WHERE\n",
    "            q.accepted_answer_id IS NOT NULL AND\n",
    "            REGEXP_CONTAINS(q.tags, \"python\") AND\n",
    "            a.creation_date >= \"{lookback_date}\"\n",
    "        LIMIT\n",
    "            10000\n",
    "        '''\n",
    "\n",
    "    def load_bq_data(query: str, client: bigquery.Client) -> pd.DataFrame:\n",
    "        \"\"\"Loads data from bq into a Pandas Dataframe for EDA.\n",
    "        Args:\n",
    "            query: BQ Query to generate data.\n",
    "            client: BQ Client used to execute query.\n",
    "        Returns:\n",
    "            pd.DataFrame: A dataframe with the requested data.\n",
    "        \"\"\"\n",
    "        df = client.query(query).to_dataframe()\n",
    "        return df\n",
    "\n",
    "    dataframe = load_bq_data(get_query(), bq_client)\n",
    "    train, test = train_test_split(dataframe, test_size=0.2)\n",
    "    train.to_json(train_data_path, orient='records', lines=True)\n",
    "    test.to_json(test_data_path, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "525abea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback_date = '2020-01-01'\n",
    "project_id = PROJECT_ID\n",
    "test_data_path = f'gs://{PROJECT_ID}-bucket/llmops/test_data.jsonl'\n",
    "train_data_path = f'gs://{PROJECT_ID}-bucket/llmops/train_data.jsonl'\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "from sklearn.model_selection import train_test_split\n",
    "bq_client = bigquery.Client(project=project_id)\n",
    "\n",
    "def get_query() -> str:\n",
    "    \"\"\"Generates BQ Query to read data.\"\"\"\n",
    "\n",
    "    return f'''SELECT\n",
    "    CONCAT(q.title, q.body) as input_text,\n",
    "    a.body AS output_text\n",
    "    FROM\n",
    "        `bigquery-public-data.stackoverflow.posts_questions` q\n",
    "    JOIN\n",
    "        `bigquery-public-data.stackoverflow.posts_answers` a\n",
    "    ON\n",
    "        q.accepted_answer_id = a.id\n",
    "    WHERE\n",
    "        q.accepted_answer_id IS NOT NULL AND\n",
    "        REGEXP_CONTAINS(q.tags, \"python\") AND\n",
    "        a.creation_date >= \"{lookback_date}\"\n",
    "    LIMIT\n",
    "        10000\n",
    "    '''\n",
    "\n",
    "def load_bq_data(query: str, client: bigquery.Client) -> pd.DataFrame:\n",
    "    \"\"\"Loads data from bq into a Pandas Dataframe for EDA.\n",
    "    Args:\n",
    "        query: BQ Query to generate data.\n",
    "        client: BQ Client used to execute query.\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe with the requested data.\n",
    "    \"\"\"\n",
    "    df = client.query(query).to_dataframe()\n",
    "    return df\n",
    "\n",
    "dataframe = load_bq_data(get_query(), bq_client)\n",
    "train, test = train_test_split(dataframe, test_size=0.2)\n",
    "train.to_json(train_data_path, orient='records', lines=True)\n",
    "test.to_json(test_data_path, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2ff2f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define an AutoMLOps component for Tuning the Foundation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e772fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@AutoMLOps.component\n",
    "def tune_model(\n",
    "    project_id: str,\n",
    "    model_display_name: str,\n",
    "    region: str,\n",
    "    train_data_path: str\n",
    "):\n",
    "    \"\"\"Custom component that prompt-tunes a foundation model.\n",
    "\n",
    "    Args:\n",
    "        project_id: The project ID.\n",
    "        model_display_name: Name of the model.\n",
    "        region: Region.\n",
    "        train_data_path: The gcs location to write the jsonl for training.\n",
    "        \n",
    "    \"\"\" \n",
    "    from google.cloud import aiplatform\n",
    "    from vertexai.preview.language_models import TextGenerationModel\n",
    "\n",
    "    aiplatform.init(project=project_id, location=region)\n",
    "    model = TextGenerationModel.from_pretrained('text-bison@001')\n",
    "\n",
    "    model.tune_model(\n",
    "        training_data=train_data_path,\n",
    "        model_display_name=model_display_name,\n",
    "        train_steps=100,\n",
    "        # Tuning can only happen in the \"europe-west4\" location\n",
    "        tuning_job_location='europe-west4',\n",
    "        # Model can only be deployed in the \"us-central1\" location\n",
    "        tuned_model_location='us-central1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c49d0458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/45373616427/locations/europe-west4/pipelineJobs/tune-large-model-20230615231642\n",
      "PipelineJob created. Resource name: projects/45373616427/locations/europe-west4/pipelineJobs/tune-large-model-20230615231642\n",
      "To use this PipelineJob in another session:\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/45373616427/locations/europe-west4/pipelineJobs/tune-large-model-20230615231642')\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/45373616427/locations/europe-west4/pipelineJobs/tune-large-model-20230615231642')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/europe-west4/pipelines/runs/tune-large-model-20230615231642?project=45373616427\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/europe-west4/pipelines/runs/tune-large-model-20230615231642?project=45373616427\n",
      "PipelineJob projects/45373616427/locations/europe-west4/pipelineJobs/tune-large-model-20230615231642 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/45373616427/locations/europe-west4/pipelineJobs/tune-large-model-20230615231642 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/45373616427/locations/europe-west4/pipelineJobs/tune-large-model-20230615231642 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/45373616427/locations/europe-west4/pipelineJobs/tune-large-model-20230615231642 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/45373616427/locations/europe-west4/pipelineJobs/tune-large-model-20230615231642 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/45373616427/locations/europe-west4/pipelineJobs/tune-large-model-20230615231642 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/45373616427/locations/europe-west4/pipelineJobs/tune-large-model-20230615231642 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/45373616427/locations/europe-west4/pipelineJobs/tune-large-model-20230615231642 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/45373616427/locations/europe-west4/pipelineJobs/tune-large-model-20230615231642 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/45373616427/locations/europe-west4/pipelineJobs/tune-large-model-20230615231642 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/45373616427/locations/europe-west4/pipelineJobs/tune-large-model-20230615231642 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/45373616427/locations/europe-west4/pipelineJobs/tune-large-model-20230615231642 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/45373616427/locations/europe-west4/pipelineJobs/tune-large-model-20230615231642 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/45373616427/locations/europe-west4/pipelineJobs/tune-large-model-20230615231642 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/45373616427/locations/europe-west4/pipelineJobs/tune-large-model-20230615231642 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/45373616427/locations/europe-west4/pipelineJobs/tune-large-model-20230615231642 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/45373616427/locations/europe-west4/pipelineJobs/tune-large-model-20230615231642 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/45373616427/locations/europe-west4/pipelineJobs/tune-large-model-20230615231642 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/45373616427/locations/europe-west4/pipelineJobs/tune-large-model-20230615231642 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/45373616427/locations/europe-west4/pipelineJobs/tune-large-model-20230615231642 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/45373616427/locations/europe-west4/pipelineJobs/tune-large-model-20230615231642 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/45373616427/locations/europe-west4/pipelineJobs/tune-large-model-20230615231642 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/45373616427/locations/europe-west4/pipelineJobs/tune-large-model-20230615231642 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/45373616427/locations/europe-west4/pipelineJobs/tune-large-model-20230615231642 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/45373616427/locations/europe-west4/pipelineJobs/tune-large-model-20230615231642 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/45373616427/locations/europe-west4/pipelineJobs/tune-large-model-20230615231642 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/45373616427/locations/europe-west4/pipelineJobs/tune-large-model-20230615231642\n",
      "PipelineJob run completed. Resource name: projects/45373616427/locations/europe-west4/pipelineJobs/tune-large-model-20230615231642\n",
      "Tuning has completed. Created Vertex Model: projects/45373616427/locations/us-central1/models/8340835834680836096\n",
      "Tuning has completed. Created Vertex Model: projects/45373616427/locations/us-central1/models/8340835834680836096\n"
     ]
    }
   ],
   "source": [
    "project_id = PROJECT_ID\n",
    "model_display_name = 'llmops-tuned-model'\n",
    "region = 'us-central1'\n",
    "train_data_path = f'gs://{PROJECT_ID}-bucket/llmops/train_data.jsonl'\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from vertexai.preview.language_models import TextGenerationModel\n",
    "\n",
    "aiplatform.init(project=project_id, location=region)\n",
    "model = TextGenerationModel.from_pretrained('text-bison@001')\n",
    "\n",
    "model.tune_model(\n",
    "    training_data=train_data_path,\n",
    "    model_display_name=model_display_name,\n",
    "    train_steps=100,\n",
    "    # Tuning can only happen in the \"europe-west4\" location\n",
    "    tuning_job_location='europe-west4',\n",
    "    # Model can only be deployed in the \"us-central1\" location\n",
    "    tuned_model_location='us-central1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ebfdcb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define a Component for Evaluating the Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e0159104",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform', \n",
    "        'pandas',\n",
    "        'rouge',\n",
    "        'sequence-evaluate',\n",
    "        'sentence-transformers'\n",
    "    ],\n",
    "    output_component_file=f'{AutoMLOps.OUTPUT_DIR}/evaluate_model.yaml',\n",
    ")\n",
    "def evaluate_model(\n",
    "    metrics: Output[Metrics],\n",
    "    model_display_name: str,\n",
    "    test_data_path: str,\n",
    "    test_dataset_size: int\n",
    "):\n",
    "    \"\"\"Custom component that evaluates the tuned model \n",
    "       and compares its performance to the foundation model.\n",
    "\n",
    "    Args:\n",
    "        model_display_name: Name of the model.\n",
    "        test_data_path: The gcs location to write the jsonl for evaluation.\n",
    "        test_dataset_size: The size of the data slice from the test dataset.\n",
    "        \n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from seq_eval import SeqEval\n",
    "    from vertexai.preview.language_models import TextGenerationModel\n",
    "\n",
    "    foundation_model = TextGenerationModel.from_pretrained('text-bison@001')\n",
    "    list_tuned_models = model.list_tuned_model_names()\n",
    "    tuned_model = TextGenerationModel.get_tuned_model(list_tuned_models[-1])\n",
    "    \n",
    "    evaluator = SeqEval()\n",
    "    \n",
    "    test_data = pd.read_json(test_data_path, lines=True)\n",
    "    \n",
    "    test_data = test_data.head(test_dataset_size)\n",
    "    test_questions = test_data['input_text']\n",
    "    test_answers = test_data['output_text']\n",
    "\n",
    "    foundation_candidates = []\n",
    "    tuned_candidates = []\n",
    "    for q in test_questions:\n",
    "        response = foundation_model.predict(q)\n",
    "        foundation_candidates.append(response.text)\n",
    "\n",
    "        response = tuned_model.predict(q)\n",
    "        tuned_candidates.append(response.text)\n",
    "    \n",
    "    references = test_answers.tolist()\n",
    "    \n",
    "    foundation_scores = evaluator.evaluate(foundation_candidates, references, verbose=False)\n",
    "    tuned_scores = evaluator.evaluate(tuned_candidates, references, verbose=False)\n",
    "    print(foundation_scores)\n",
    "    print(tuned_scores)\n",
    "    \n",
    "    # ADD IN METRICS PART\n",
    "    \n",
    "    # ADD IN PREDICTION PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e17e01b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b39b76e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/45373616427/locations/us-central1/models/8340835834680836096'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "list_tuned_models = model.list_tuned_model_names()\n",
    "list_tuned_models[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "5f97f64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu_1': 0.10458544022959128, 'bleu_2': 0.04248843653127929, 'bleu_3': 0.02345984191215938, 'bleu_4': 0.014390707453807598, 'rouge_1_precision': 0.25326673070468825, 'rouge_1_recall': 0.17789095911876043, 'rouge_1_f1': 0.18223478361172607, 'rouge_2_precision': 0.055061102765300844, 'rouge_2_recall': 0.04190439405705797, 'rouge_2_f1': 0.03836401633965376, 'rouge_l_precision': 0.23505666853683077, 'rouge_l_recall': 0.16574717607271883, 'rouge_l_f1': 0.1690684979609612, 'inter_dist1': 0.0015170786273477207, 'inter_dist2': 0.03172384126838158, 'intra_dist1': 0.11913427505604801, 'intra_dist2': 0.42174181634607605, 'semantic_textual_similarity': 0.5751550720959175}\n",
      "{'bleu_1': 0.10458544022959128, 'bleu_2': 0.04248843653127929, 'bleu_3': 0.02345984191215938, 'bleu_4': 0.014390707453807598, 'rouge_1_precision': 0.25326673070468825, 'rouge_1_recall': 0.17789095911876043, 'rouge_1_f1': 0.18223478361172607, 'rouge_2_precision': 0.055061102765300844, 'rouge_2_recall': 0.04190439405705797, 'rouge_2_f1': 0.03836401633965376, 'rouge_l_precision': 0.23505666853683077, 'rouge_l_recall': 0.16574717607271883, 'rouge_l_f1': 0.1690684979609612, 'inter_dist1': 0.0015170786273477207, 'inter_dist2': 0.03172384126838158, 'intra_dist1': 0.11913427505604801, 'intra_dist2': 0.42174181634607605, 'semantic_textual_similarity': 0.5751550720959175}\n"
     ]
    }
   ],
   "source": [
    "test_data_path = f'gs://{PROJECT_ID}-bucket/llmops/test_data.jsonl'\n",
    "test_dataset_size = 200\n",
    "\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from seq_eval import SeqEval\n",
    "from vertexai.preview.language_models import TextGenerationModel\n",
    "\n",
    "foundation_model = TextGenerationModel.from_pretrained('text-bison@001')\n",
    "list_tuned_models = model.list_tuned_model_names()\n",
    "tuned_model = TextGenerationModel.get_tuned_model(list_tuned_models[-1])\n",
    "\n",
    "evaluator = SeqEval()\n",
    "\n",
    "test_data = pd.read_json(test_data_path, lines=True)\n",
    "\n",
    "test_data = test_data.head(test_dataset_size)\n",
    "test_questions = test_data['input_text']\n",
    "test_answers = test_data['output_text']\n",
    "\n",
    "foundation_candidates = []\n",
    "tuned_candidates = []\n",
    "references = []\n",
    "for i in range(len(test_questions)):\n",
    "    response_a = foundation_model.predict(re.sub(r'\\<.*?\\>', '', test_questions[i]))\n",
    "    response_b = tuned_model.predict(test_questions[i])\n",
    "    if response_a.text != '' and response_b.text != '':\n",
    "        references.append(re.sub(r'\\<.*?\\>', '', test_answers[i]))\n",
    "        foundation_candidates.append(response_a.text)\n",
    "        tuned_candidates.append(response_b.text)\n",
    "\n",
    "foundation_scores = evaluator.evaluate(foundation_candidates, references, verbose=False)\n",
    "tuned_scores = evaluator.evaluate(tuned_candidates, references, verbose=False)\n",
    "\n",
    "print(foundation_scores)\n",
    "print(tuned_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "3f8d03da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************\n",
      "* BLEU SCORES *\n",
      "***************\n",
      "BLEU-1:  0.1080129638876945\n",
      "BLEU-2:  0.04915338038875601\n",
      "BLEU-3:  0.028422402691031894\n",
      "BLEU-4:  0.018621638960148627\n",
      "\n",
      "\n",
      "****************\n",
      "* ROUGE SCORES *\n",
      "****************\n",
      "ROUGE-1 PRECISION:  0.2773307474734131\n",
      "ROUGE-1 RECALL:  0.184434217635193\n",
      "ROUGE-1 F1 :  0.19296703212373456\n",
      "\n",
      "\n",
      "ROUGE-2 PRECISION:  0.08890513795264643\n",
      "ROUGE-2 RECALL:  0.0539914623781837\n",
      "ROUGE-2 F1 :  0.05600351238983955\n",
      "\n",
      "\n",
      "ROUGE-L PRECISION:  0.25896537445455853\n",
      "ROUGE-L RECALL:  0.1709123557727651\n",
      "ROUGE-L F1 :  0.17881360611799996\n",
      "\n",
      "\n",
      "*********************\n",
      "* DISTINCT-N SCORES *\n",
      "*********************\n",
      "INTER DIST-1:  0.0015272276043960086\n",
      "INTER DIST-2:  0.03548927346112302\n",
      "INTRA DIST-1:  0.11903927570898948\n",
      "INTRA DIST-2:  0.43035543407568877\n",
      "\n",
      "\n",
      "******************************************************\n",
      "* SEMANTIC TEXTUAL SIMILARITY (Sentence Transformer) *\n",
      "******************************************************\n",
      "COSINE SIMILARITY:  0.6128882431402439\n",
      "{'bleu_1': 0.1080129638876945, 'bleu_2': 0.04915338038875601, 'bleu_3': 0.028422402691031894, 'bleu_4': 0.018621638960148627, 'rouge_1_precision': 0.2773307474734131, 'rouge_1_recall': 0.184434217635193, 'rouge_1_f1': 0.19296703212373456, 'rouge_2_precision': 0.08890513795264643, 'rouge_2_recall': 0.0539914623781837, 'rouge_2_f1': 0.05600351238983955, 'rouge_l_precision': 0.25896537445455853, 'rouge_l_recall': 0.1709123557727651, 'rouge_l_f1': 0.17881360611799996, 'inter_dist1': 0.0015272276043960086, 'inter_dist2': 0.03548927346112302, 'intra_dist1': 0.11903927570898948, 'intra_dist2': 0.43035543407568877, 'semantic_textual_similarity': 0.6128882431402439}\n"
     ]
    }
   ],
   "source": [
    "foundation_scores = evaluator.evaluate(foundation_candidates, references, verbose=True)\n",
    "print(foundation_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "93aac67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************\n",
      "* BLEU SCORES *\n",
      "***************\n",
      "BLEU-1:  0.10458544022959128\n",
      "BLEU-2:  0.04248843653127929\n",
      "BLEU-3:  0.02345984191215938\n",
      "BLEU-4:  0.014390707453807598\n",
      "\n",
      "\n",
      "****************\n",
      "* ROUGE SCORES *\n",
      "****************\n",
      "ROUGE-1 PRECISION:  0.25326673070468825\n",
      "ROUGE-1 RECALL:  0.17789095911876043\n",
      "ROUGE-1 F1 :  0.18223478361172607\n",
      "\n",
      "\n",
      "ROUGE-2 PRECISION:  0.055061102765300844\n",
      "ROUGE-2 RECALL:  0.04190439405705797\n",
      "ROUGE-2 F1 :  0.03836401633965376\n",
      "\n",
      "\n",
      "ROUGE-L PRECISION:  0.23505666853683077\n",
      "ROUGE-L RECALL:  0.16574717607271883\n",
      "ROUGE-L F1 :  0.1690684979609612\n",
      "\n",
      "\n",
      "*********************\n",
      "* DISTINCT-N SCORES *\n",
      "*********************\n",
      "INTER DIST-1:  0.0015170786273477207\n",
      "INTER DIST-2:  0.03172384126838158\n",
      "INTRA DIST-1:  0.11913427505604801\n",
      "INTRA DIST-2:  0.42174181634607605\n",
      "\n",
      "\n",
      "******************************************************\n",
      "* SEMANTIC TEXTUAL SIMILARITY (Sentence Transformer) *\n",
      "******************************************************\n",
      "COSINE SIMILARITY:  0.5751550720959175\n",
      "{'bleu_1': 0.10458544022959128, 'bleu_2': 0.04248843653127929, 'bleu_3': 0.02345984191215938, 'bleu_4': 0.014390707453807598, 'rouge_1_precision': 0.25326673070468825, 'rouge_1_recall': 0.17789095911876043, 'rouge_1_f1': 0.18223478361172607, 'rouge_2_precision': 0.055061102765300844, 'rouge_2_recall': 0.04190439405705797, 'rouge_2_f1': 0.03836401633965376, 'rouge_l_precision': 0.23505666853683077, 'rouge_l_recall': 0.16574717607271883, 'rouge_l_f1': 0.1690684979609612, 'inter_dist1': 0.0015170786273477207, 'inter_dist2': 0.03172384126838158, 'intra_dist1': 0.11913427505604801, 'intra_dist2': 0.42174181634607605, 'semantic_textual_similarity': 0.5751550720959175}\n"
     ]
    }
   ],
   "source": [
    "tuned_scores = evaluator.evaluate(tuned_candidates, references, verbose=True)\n",
    "print(tuned_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "80b3ad8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modifying a string under a specific conditionIn a Python program, I am trying to modify a string under a specific condition:\n",
      "X = ('4c0')\n",
      "Sig = ['a', 'b', 'c', 'e']\n",
      "\n",
      "Sig is a list. Additionally, I have a tuple:\n",
      "T = (4,'d',5)\n",
      "\n",
      "If the second element (T[1]) is not in Sig, I must create another string, starting from X:\n",
      "\n",
      "as T[1] ('d') is not in Sig, T[2] must replace T[0] in X ('5' replacing '4');\n",
      "the last element in X must be added by 1 ('1' replacing '0').\n",
      "\n",
      "In this case, the desired result should be:\n",
      "Y = ('5c1')\n",
      "\n",
      "I made this code but it is not add any string to Y:\n",
      "Y = []\n",
      "for i in TT: # TT has the tuple T\n",
      "    i = list(i)\n",
      "    if i[1] not in Sig:\n",
      "        for j in TT:\n",
      "            if type(j[2]) == str:\n",
      "                if i[1] == j[1]:\n",
      "                    Y.append(j[2][0]+i[1]+str(int(j[2][2]+1)))\n",
      "\n",
      "Any ideas how I could solve this problem?\n",
      "-\n",
      "I think the problem is that you are not iterating over the list of tuples correctly. You should iterate over the list of tuples, and for each tuple, you should iterate over the elements of the tuple. In your code, you are only iterating over the first element of the tuple.\n",
      "\n",
      "Here is a corrected version of your code:\n",
      "\n",
      "```\n",
      "Y = []\n",
      "for i in TT:\n",
      "    i = list(i)\n",
      "    if i[1] not in Sig:\n",
      "        for j in TT:\n",
      "            if type(j[2]) == str:\n",
      "                if\n",
      "-\n",
      "The code you provided is not correct. The following code should work:\n",
      "\n",
      "```\n",
      "X = '4c0'\n",
      "Sig = ['a', 'b', 'c', 'e']\n",
      "T = (4, 'd', 5)\n",
      "Y = []\n",
      "for i in T:\n",
      "    i = list(i)\n",
      "    if i[1] not in Sig:\n",
      "        for j in T:\n",
      "            if type(j[2]) == str:\n",
      "                if i[1] == j[1]:\n",
      "                    Y\n",
      "-\n",
      "There are a lot of missing conditions in your problem statement (e.g. what to do if 'd' was in Sig, How to handle values past 9, what if X[0] is not the same as T[0], what if X[2] is equal to T[2])\n",
      "For the example given a simple string format should suffice:\n",
      "X   = ('4c0')\n",
      "Sig = ['a', 'b', 'c', 'e']\n",
      "T   = (4,'d',5)\n",
      "\n",
      "if T[1] not in Sig:\n",
      "    Y = f&quot;{T[2]}{X[1]}{int(X[2])+1}&quot;\n",
      "\n",
      "print(Y) # 5c1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(re.sub(r'\\<.*?\\>', '', test_questions[1]))\n",
    "print('-')\n",
    "print(foundation_candidates[1])\n",
    "print('-')\n",
    "print(tuned_candidates[1])\n",
    "print('-')\n",
    "print(references[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453d1f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1d8fb31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nltk.word_tokenize(references[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b60064d2",
   "metadata": {},
   "outputs": [],
   "source": [
    " b = nltk.word_tokenize(tuned_candidates[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b03230d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0450128389486753e-78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srastatter/Library/Python/3.9/lib/python/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "hypothesis = ['It', 'is', 'a', 'cat', 'at', 'room']\n",
    "reference = ['It', 'is', 'a', 'cat', 'inside', 'the', 'room']\n",
    "#there may be several references\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu(a, b)\n",
    "print(BLEUscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00472729",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee83f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62bdafcf-dafe-461b-a161-b1db0ea1062c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define an AutoMLOps pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "26726ec7-3021-438e-9d60-000cd89483b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@AutoMLOps.pipeline(\n",
    "    name='bqml-automlops-retail-forecasting',\n",
    "    description='This is an example of retail demand forecasting using AutoMLOps and BQML.')\n",
    "def pipeline(confidence_lvl: float,\n",
    "             dataset_id: str,\n",
    "             forecast_horizon: int,\n",
    "             machine_type: str,\n",
    "             model_name: str,\n",
    "             model_type: str,\n",
    "             project_id: str,\n",
    "             sales_table: str,\n",
    "             year_range: int):\n",
    "    \n",
    "    prepare_sales_table_task = prepare_sales_table(\n",
    "        dataset_id=dataset_id,\n",
    "        project_id=project_id,\n",
    "        sales_table=sales_table)        \n",
    "\n",
    "    create_training_dataset_task = create_training_dataset(\n",
    "        dataset_id=dataset_id,\n",
    "        project_id=project_id,\n",
    "        sales_table=sales_table,\n",
    "        year_range=year_range).after(prepare_sales_table_task)\n",
    "\n",
    "    train_model_task = train_model(\n",
    "        dataset_id=dataset_id,\n",
    "        model_name=model_name,\n",
    "        model_type=model_type,\n",
    "        project_id=project_id).after(create_training_dataset_task)\n",
    "\n",
    "    evaluate_model_task = evaluate_model(\n",
    "        dataset_id=dataset_id,\n",
    "        model_name=model_name,\n",
    "        project_id=project_id).after(train_model_task)\n",
    "    \n",
    "    forecast_task = forecast(\n",
    "        confidence_lvl=confidence_lvl,\n",
    "        dataset_id=dataset_id,\n",
    "        forecast_horizon=forecast_horizon,\n",
    "        project_id=project_id).after(evaluate_model_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3874f4f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define the Pipeline Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4cb3786b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_params = {\n",
    "    'confidence_lvl': 0.90,\n",
    "    'dataset_id': dataset_id,\n",
    "    'forecast_horizon': 90,\n",
    "    'machine_type': 'n1-standard-4',\n",
    "    'model_name': 'arima_model',\n",
    "    'model_type': 'ARIMA_PLUS',\n",
    "    'project_id': PROJECT_ID,\n",
    "    'sales_table': SALES_TABLE,\n",
    "    'year_range': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e0aa05",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generate and Run the pipeline\n",
    "`AutoMLOps.generate` generates the code for the MLOps pipeline. `AutoMLOps.go` generates the code and runs the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "db51a8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Successfully saved requirements file in AutoMLOps/components/component_base/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "AutoMLOps.generate(project_id=PROJECT_ID,\n",
    "                   pipeline_params=pipeline_params,\n",
    "                   run_local=False,\n",
    "                   schedule_pattern='59 11 * * 0' # retrain every Sunday at Midnight\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7ef279e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Successfully saved requirements file in AutoMLOps/components/component_base/requirements.txt\n",
      "\u001b[0;32m Updating required API services in project automlops-sandbox \u001b[0m\n",
      "Operation \"operations/acat.p2-45373616427-e2045dc0-8a44-42d2-90bb-e636c7d6b101\" finished successfully.\n",
      "\u001b[0;32m Checking for Artifact Registry: vertex-mlops-af in project automlops-sandbox \u001b[0m\n",
      "Listing items under project automlops-sandbox, location us-central1.\n",
      "\n",
      "vertex-mlops-af  DOCKER  STANDARD_REPOSITORY  Artifact Registry vertex-mlops-af in us-central1.  us-central1          Google-managed key  2023-01-11T22:12:26  2023-06-14T13:36:39  59728.422\n",
      "Artifact Registry: vertex-mlops-af already exists in project automlops-sandbox\n",
      "\u001b[0;32m Checking for GS Bucket: automlops-sandbox-bucket in project automlops-sandbox \u001b[0m\n",
      "gs://automlops-sandbox-bucket/\n",
      "GS Bucket: automlops-sandbox-bucket already exists in project automlops-sandbox\n",
      "\u001b[0;32m Checking for Service Account: vertex-pipelines in project automlops-sandbox \u001b[0m\n",
      "Pipeline Runner Service Account         vertex-pipelines@automlops-sandbox.iam.gserviceaccount.com  False\n",
      "Service Account: vertex-pipelines already exists in project automlops-sandbox\n",
      "\u001b[0;32m Updating required IAM roles in project automlops-sandbox \u001b[0m\n",
      "\u001b[0;32m Checking for Cloud Source Repository: AutoMLOps-repo in project automlops-sandbox \u001b[0m\n",
      "AutoMLOps-repo  automlops-sandbox  https://source.developers.google.com/p/automlops-sandbox/r/AutoMLOps-repo\n",
      "Cloud Source Repository: AutoMLOps-repo already exists in project automlops-sandbox\n",
      "\u001b[0;32m Checking for Cloud Tasks Queue: queueing-svc in project automlops-sandbox \u001b[0m\n",
      "queueing-svc       RUNNING  1000              500.0            100\n",
      "Cloud Tasks Queue: queueing-svc already exists in project automlops-sandbox\n",
      "\u001b[0;32m Checking for Cloudbuild Trigger: automlops-trigger in project automlops-sandbox \u001b[0m\n",
      "name: automlops-trigger\n",
      "Cloudbuild Trigger already exists in project automlops-sandbox for repo AutoMLOps-repo\n",
      "[automlops 4213d1a] Run AutoMLOps\n",
      " 2 files changed, 1587 insertions(+), 1407 deletions(-)\n",
      "To https://source.developers.google.com/p/automlops-sandbox/r/AutoMLOps-repo\n",
      "   a28f12d..4213d1a  automlops -> automlops\n",
      "Pushing code to automlops branch, triggering cloudbuild...\n",
      "Cloudbuild job running at: https://console.cloud.google.com/cloud-build/builds;region=us-central1\n",
      "\n",
      "#################################################################\n",
      "#                                                               #\n",
      "#                       RESOURCES MANIFEST                      #\n",
      "#---------------------------------------------------------------#\n",
      "#     Generated resources can be found at the following urls    #\n",
      "#                                                               #\n",
      "#################################################################\n",
      "\n",
      "Google Cloud Storage Bucket: https://console.cloud.google.com/storage/automlops-sandbox-bucket\n",
      "Artifact Registry: https://console.cloud.google.com/artifacts/docker/automlops-sandbox/us-central1/vertex-mlops-af\n",
      "Service Accounts: https://console.cloud.google.com/iam-admin/serviceaccounts?project=automlops-sandbox\n",
      "APIs: https://console.cloud.google.com/apis\n",
      "Cloud Source Repository: https://source.cloud.google.com/automlops-sandbox/AutoMLOps-repo/+/automlops:\n",
      "Cloud Build Jobs: https://console.cloud.google.com/cloud-build/builds;region=us-central1\n",
      "Vertex AI Pipeline Runs: https://console.cloud.google.com/vertex-ai/pipelines/runs\n",
      "Cloud Build Trigger: https://console.cloud.google.com/cloud-build/triggers;region=us-central1\n",
      "Cloud Run Service: https://console.cloud.google.com/run/detail/us-central1/run-pipeline\n",
      "Cloud Tasks Queue: https://console.cloud.google.com/cloudtasks/queue/us-central1/queueing-svc/tasks\n",
      "Cloud Scheduler Job: https://console.cloud.google.com/cloudscheduler\n"
     ]
    }
   ],
   "source": [
    "AutoMLOps.go(project_id=PROJECT_ID,\n",
    "             pipeline_params=pipeline_params,\n",
    "             run_local=False,\n",
    "             schedule_pattern='59 11 * * 0'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28122ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m102"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
